{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_token_dict = {\n",
    "    'GleghornLab/abstract_domain_copd': '[COPD]',\n",
    "    'GleghornLab/abstract_domain_cvd': '[CVD]',\n",
    "    'GleghornLab/abstract_domain_skincancer': '[CANCER]',\n",
    "    'GleghornLab/abstract_domain_parasitic': '[PARASITIC]',\n",
    "    'GleghornLab/abstract_domain_autoimmune': '[AUTOIMMUNE]'\n",
    "}\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# for each dataset, switch the valid and test splits\n",
    "\n",
    "for path, _ in path_token_dict.items():\n",
    "    data = load_dataset(path)\n",
    "    valid, test = data['valid'], data['test']\n",
    "    data['test'] = valid\n",
    "    data['valid'] = test\n",
    "    data.push_to_hub(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]Hello, world![SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello, world!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = mteb.Benchmark(\n",
    "    name='MTEB(Medical)',\n",
    "    tasks=mteb.get_tasks(\n",
    "        tasks=['Banking77Classification']\n",
    "        )\n",
    ")\n",
    "benchmark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.get_data import get_single_train_data, SimDataset\n",
    "from models.utils import add_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "path_token_dict = {\n",
    "    'GleghornLab/abstract_domain_copd': '[COPD]',\n",
    "    'GleghornLab/abstract_domain_cvd': '[CVD]',\n",
    "    'GleghornLab/abstract_domain_skincancer': '[CANCER]',\n",
    "    'GleghornLab/abstract_domain_parasitic': '[PARASITIC]',\n",
    "    'GleghornLab/abstract_domain_autoimmune': '[AUTOIMMUNE]'\n",
    "}\n",
    "\n",
    "token_expert_dict = {\n",
    "    '[COPD]': 0,\n",
    "    '[CVD]': 1,\n",
    "    '[CANCER]': 2,\n",
    "    '[PARASITIC]': 3,\n",
    "    '[AUTOIMMUNE]': 4\n",
    "}\n",
    "\n",
    "\n",
    "model, tokenizer = add_new_tokens(model, tokenizer, list(path_token_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_single_train_data(\n",
    "    data_path='GleghornLab/abstract_domain_cvd',\n",
    "    tokenizer=tokenizer,\n",
    "    path_token_dict=path_token_dict,\n",
    "    token_expert_dict=token_expert_dict,\n",
    "    max_length=512,\n",
    "    add_tokens=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(path_token_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from typing import Any, List, Dict\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def get_all_train_data(\n",
    "    data_paths: List[str],\n",
    "    tokenizer: Any,\n",
    "    path_token_dict: Dict[str, str],\n",
    "    token_expert_dict: Dict[str, int],\n",
    "    max_length: int = 512,\n",
    "    add_tokens: bool = True,\n",
    "):\n",
    "    all_a_documents, all_b_documents, all_expert_assignments = [], [], []\n",
    "    for path in data_paths:\n",
    "        domain_token = path_token_dict[path]\n",
    "        expert_assignment = token_expert_dict[domain_token]\n",
    "        data = load_dataset(path, split='train').select(range(100))\n",
    "        all_a_documents.extend(data['a'])\n",
    "        all_b_documents.extend(data['b'])\n",
    "        all_expert_assignments.extend([expert_assignment] * len(data['a']))\n",
    "\n",
    "    random.seed(42)\n",
    "    entries = list(zip(all_a_documents, all_b_documents, all_expert_assignments))\n",
    "    random.shuffle(entries)\n",
    "    all_a_documents, all_b_documents, all_expert_assignments = zip(*entries)\n",
    "    domain_tokens = list(path_token_dict.values())\n",
    "    dataset = SimDataset(\n",
    "        a_documents=all_a_documents,\n",
    "        b_documents=all_b_documents,\n",
    "        expert_assignments=all_expert_assignments,\n",
    "        domain_tokens=domain_tokens,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        add_tokens=add_tokens\n",
    "\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_all_train_data(\n",
    "    data_paths=list(path_token_dict.keys()),\n",
    "    tokenizer=tokenizer,\n",
    "    path_token_dict=path_token_dict,\n",
    "    token_expert_dict=token_expert_dict,\n",
    "    max_length=512,\n",
    "    add_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for k, v in dataset[i].items():\n",
    "        if 'doc' in k:\n",
    "            print(k)\n",
    "            print(tokenizer.decode(v['input_ids'][0]))\n",
    "        else:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
