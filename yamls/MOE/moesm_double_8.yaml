general_args:
  model_path: 'facebook/esm2_t6_8M_UR50D'
  ESM: true # ESM or BERT
  model_type: 'Double'
  contact_head: false
  huggingface_username: lhallee

# MOE
  single_moe: false
  moe_type: 'topk'
  MOE: true
  token_moe: true
  num_experts: 8  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 4  # Number of tasks for MulitTask learning
  expert_loss: false
  MI: false
  BAL: false
  wBAL: !!float 0.1
  wEX: !!float 0.1

# Model settings
  hidden_dropout_prob: 0.05  # Dropout rate for the model
  attention_probs_dropout_prob: 0.0

# New tokens
  new_special_tokens: false  # Add new special tokens for each domain token seeded with CLS
  domains: # List of domain tags
    - '[EC]'
    - '[BP]'
    - '[CC]'
    - '[MF]'
    - '[IP]'
    - '[3D]'

# data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/ProteinDouble50'
  a_col: 'a'  # First feature column name in datasets # doubles as anchor
  b_col: 'b'  # Second feature column name in datasets
  p_col: 'positives'
  n_col: 'negatives'
  label_col: 'aspects'
  max_length: 16 # Maximum length of the sequences
  num_labels: 2
  valid_size: 10000
  test_size: 100000
  trim: true

# wandb settings
  wandb: false
  wandb_project: SSPR
  wandb_name: triplet_test

# Paths
  log_path: './results_moesm_double_late.txt'  # Path to save the log file
  weight_path: null  # Path to the model weights to load
  save_path: 'moesm_double.pt'

# Training settings
  patience: 100

# Eval settings
  limits: false  # Lets user define limits for F1max

# Training arguments for HF trainer
training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 1
  per_device_eval_batch_size: !!int 1
  gradient_accumulation_steps: !!int 16
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 10
  warmup_steps: !!int 100
  save_strategy: !!str steps
  save_steps: !!int 1000
  save_total_limit: !!int 3
  evaluation_strategy: !!str steps
  eval_steps: 1000
  logging_strategy: steps
  logging_steps: !!int 1
  fp16: false
  eval_accumulation_steps: 500
  group_by_length: false
  length_column_name: !!str length
  metric_for_best_model: !!str loss

# Training arugments for HF trainer during evaluation
eval_training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 64
  per_device_eval_batch_size: !!int 64
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-4
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 200
  warmup_steps: !!int 100
  save_strategy: !!str epoch
  save_total_limit: !!int 3
  evaluation_strategy: !!str epoch
  logging_strategy: epoch
  fp16: false
  seed: !!int 7
  metric_for_best_model: !!str loss

# General eval args during evaluation
eval_args:

# Paths
  data_paths:
    - 'lhallee/EC_reg'
    - 'lhallee/CC_reg'
    - 'lhallee/MF_reg'
    - 'lhallee/BP_reg'
    - 'lhallee/dl_binary_reg'
    - 'lhallee/dl_ten_reg'
    - 'lhallee/MetalIonBinding_reg'
  weight_path: null
  log_path: ./results_moesm_double_late_downstream.txt
  db_path: moesm_double_late.db

# New tokens
  domains:
    - '[EC]'
    - '[CC]'
    - '[MF]'
    - '[BP]'
    - '[CC]'
    - '[CC]'
    - '[IP]'

# Model settings
  input_dim: 768
  hidden_dim: 8096
  dropout: 0.1
  num_layers: 2
  nhead: 8
  num_labels: 4

# Data settings
  trim: false
  max_length: 512
  skip: false # skip embedding, already embedded
  pooling: 'mean'
  read_scaler: 100
  full: false
  new_special_tokens: false
  sql: true

# Trainging settings
  patience: 10