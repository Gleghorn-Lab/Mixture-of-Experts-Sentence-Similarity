general_args:
  model_path: 'facebook/esm2_t6_8M_UR50D'
  ESM: true # ESM or BERT
  model_type: 'Triplet'

  moe_type: 'topk'
  MOE: true 
  token_moe: false

  contact_head: false

  num_experts: 4  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 2  # Number of tasks for MulitTask learning
  hidden_dropout_prob: 0.05  # Dropout rate for the model
  contrastive_loss: clip
  MI_loss: false

  add_during_eval: false # add tokens during eval
  new_special_tokens: false  # Add new special tokens for each domain token seeded with CLS
  domains:  # List of domain tags
    - '[TEST]'
  
  wBAL: !!float 0.1
  wMI: !!float 0.1

#data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/MetalIonBinding_reg'
  a_col: 'seqs'  # First feature column name in datasets
  b_col: 'seqs'  # Second feature column name in datasets
  label_col: 'labels'  # Label column name in datasets
  max_length: 8  # Maximum length of the sequences
  num_labels: 2
  valid_size: 5000
  test_size: 5000

#id_settings:
  wandb_api_key: null # Weights and biases API key
  hf_token: null # Hugging Face API key
  hf_username: 'lhallee'  # Hugging Face username

#misc:
  project_name: 'SciMOE'  # Name of the project
  log_path: './results.txt'  # Path to save the log file
  weight_path: null  # Path to the model weights to load

  patience: 10
  limits: false  # Lets user define limits for F1max


training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs

  per_device_train_batch_size: !!int 2
  per_device_eval_batch_size: !!int 2
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01

  num_train_epochs: !!int 10
  warmup_ratio: !!float 0.0
  warmup_steps: !!int 500

  save_strategy: !!str steps
  save_steps: !!int 2500
  save_total_limit: !!int 3

  evaluation_strategy: !!str steps
  eval_steps: 100

  logging_strategy: steps
  logging_steps: !!int 100

  bf16: false
  fp16: false

  seed: !!int 42

  eval_accumulation_steps: null
  group_by_length: false
  length_column_name: !!str length
  save_safetensors: true
  metric_for_best_model: !!str loss


eval_training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs

  per_device_train_batch_size: !!int 64
  per_device_eval_batch_size: !!int 64
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-4
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01

  num_train_epochs: !!int 200
  warmup_steps: !!int 100

  save_strategy: !!str epoch
  save_total_limit: !!int 3

  evaluation_strategy: !!str epoch

  logging_strategy: steps
  logging_steps: !!int 100

  bf16: false
  fp16: false

  seed: !!int 42

  save_safetensors: true
  metric_for_best_model: !!str loss


eval_args:
plm_path: facebook/esm2_t6_8M_UR50D
data_paths:
  - ''
weight_path: ''
log_path: ./results.txt
output_dir: ./trainer_output
db_path: embeddings.db
input_dim: 768
hidden_dim: 768
intermediate_dim: 2048
dropout: 0.1
num_layers: 2
nhead: 8
trim: false
max_length: 512
pooling: mean
lr: 0.0001
batch_size: 2
grad_accum: 1
weight_decay: 0.01
fp16: false
epochs: 200
patience: 10
seed: 7
skip: false # skip embedding, already embedded
logging_steps: 100