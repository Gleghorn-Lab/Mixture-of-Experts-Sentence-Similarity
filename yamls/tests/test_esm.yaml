general_args:
  model_path: 'facebook/esm2_t6_8M_UR50D'
  ESM: true # ESM or BERT
  model_type: 'Triplet'

  moe_type: 'topk'
  MOE: true 
  token_moe: true

  contact_head: false

  num_experts: 4  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 7  # Number of tasks for MulitTask learning
  hidden_dropout_prob: 0.05  # Dropout rate for the model
  contrastive_loss: clip
  MI_loss: true

  add_during_eval: false # add tokens during eval
  new_special_tokens: true  # Add new special tokens for each domain token seeded with CLS
  domains: # List of domain tags
    - '[EC]'
    - '[CO]'
    - '[BP]'
    - '[CC]'
    - '[MF]'
    - '[IP]'
    - '[3D]'
  
  wBAL: !!float 0.01
  wMI: !!float 0.05

#data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/triplets'
  a_col: 'anchors'  # First feature column name in datasets # doubles as anchor
  b_col: 'seqs'  # Second feature column name in datasets
  p_col: 'positives'
  n_col: 'negatives'
  label_col: 'aspects'
  max_length: 8  # Maximum length of the sequences
  num_labels: 2
  valid_size: 100
  test_size: 100

#wandb settings
  wandb: false
  wandb_project: SSPR
  wandb_name: triplet_test


  log_path: './results.txt'  # Path to save the log file
  weight_path: null  # Path to the model weights to load

  patience: 0
  limits: false  # Lets user define limits for F1max


training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 2
  per_device_eval_batch_size: !!int 2
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_ratio: !!float 0.0
  warmup_steps: !!int 500
  save_strategy: !!str steps
  save_steps: !!int 2500
  save_total_limit: !!int 3
  evaluation_strategy: !!str steps
  eval_steps: 10
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: false
  seed: !!int 42
  eval_accumulation_steps: null
  group_by_length: false
  length_column_name: !!str length
  metric_for_best_model: !!str loss


eval_training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 64
  per_device_eval_batch_size: !!int 64
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-4
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_steps: !!int 100
  save_strategy: !!str epoch
  save_total_limit: !!int 3
  evaluation_strategy: !!str epoch
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: true
  seed: !!int 7
  metric_for_best_model: !!str loss


eval_args:
  plm_path: facebook/esm2_t6_8M_UR50D
  data_paths:
    - 'lhallee/EC_reg'
    - 'lhallee/CC_reg'
    - 'lhallee/MF_reg'
    - 'lhallee/BP_reg'
    - 'lhallee/dl_binary_reg'
    - 'lhallee/dl_ten_reg'
    - 'lhallee/MetalIonBinding_reg'
  domains:
    - '[EC]'
    - '[CC]'
    - '[MF]'
    - '[BP]'
    - '[CC]'
    - '[CC]'
    - '[IP]'

  weight_path: ''
  log_path: ./results.txt
  output_dir: ./trainer_output
  db_path: embeddings.db
  input_dim: 768
  hidden_dim: 768
  intermediate_dim: 2048
  dropout: 0.1
  num_layers: 2
  nhead: 8
  trim: false
  max_length: 512
  patience: 1
  skip: false # skip embedding, already embedded
  logging_steps: 100