{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "from data.load_data import get_fine_tune_data, get_seqs\n",
    "from models.protein_vec.src_run.model_protein_moe import trans_basic_block, trans_basic_block_Config\n",
    "from models.protein_vec.src_run.utils_search import *\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model\n",
    "orig_cwd = os.getcwd()\n",
    "\n",
    "vec_model_cpnt = 'protein_vec_models/protein_vec.ckpt'\n",
    "vec_model_config = 'protein_vec_models/protein_vec_params.json'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"lhallee/prot_t5_enc\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"lhallee/prot_t5_enc\").half().to(device).eval()\n",
    "\n",
    "os.chdir('models/protein_vec/src_run/')\n",
    "vec_model_config = trans_basic_block_Config.from_json(vec_model_config)\n",
    "model_deep = trans_basic_block.load_from_checkpoint(vec_model_cpnt, config=vec_model_config).to(device).eval()\n",
    "\n",
    "os.chdir(orig_cwd)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding\n",
    "\n",
    "def embed_seqs(model, model_deep, tokenizer, seqs, device):\n",
    "    sampled_keys = np.array(['TM', 'PFAM', 'GENE3D', 'ENZYME', 'MFO', 'BPO', 'CCO'])\n",
    "    all_cols = np.array(['TM', 'PFAM', 'GENE3D', 'ENZYME', 'MFO', 'BPO', 'CCO'])\n",
    "    masks = [all_cols[k] in sampled_keys for k in range(len(all_cols))]\n",
    "    masks = torch.logical_not(torch.tensor(masks, dtype=torch.bool))[None,:]\n",
    "\n",
    "    embed_all_sequences = []\n",
    "    for seq in tqdm(seqs, desc='Embedding'): \n",
    "        protrans_sequence = featurize_prottrans([seq], model, tokenizer, device)\n",
    "        embedded_sequence = embed_vec(protrans_sequence, model_deep, masks, device)\n",
    "        embed_all_sequences.append(embedded_sequence)\n",
    "    return np.concatenate(embed_all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get seqs\n",
    "\n",
    "dataset_paths = [\n",
    "    'lhallee/EC_reg',\n",
    "    'lhallee/CC_reg',\n",
    "    'lhallee/MF_reg',\n",
    "    'lhallee/BP_reg',\n",
    "    'lhallee/dl_binary_reg',\n",
    "    'lhallee/dl_ten_reg',\n",
    "    'lhallee/MetalIonBinding_reg'\n",
    "]\n",
    "\n",
    "class args:\n",
    "    trim=True\n",
    "    max_length=1024\n",
    "\n",
    "\n",
    "datasets, all_seqs, train_sets, valid_sets, test_sets, num_labels, task_types = [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "for i, data_path in enumerate(dataset_paths):\n",
    "    train_set, valid_set, test_set, num_label, task_type = get_fine_tune_data(args, data_path)\n",
    "\n",
    "    train_seqs, train_labels = get_seqs(train_set)\n",
    "    valid_seqs, valid_labels = get_seqs(valid_set)\n",
    "    test_seqs, test_labels = get_seqs(test_set)\n",
    "\n",
    "    train_seqs, train_labels = train_seqs, train_labels\n",
    "    valid_seqs, valid_labels = valid_seqs, valid_labels\n",
    "    test_seqs, test_labels = test_seqs, test_labels\n",
    "\n",
    "    train_sets.append((train_seqs, train_labels))\n",
    "    valid_sets.append((valid_seqs, valid_labels))\n",
    "    test_sets.append((test_seqs, test_labels))\n",
    "\n",
    "    all_seqs.extend(train_seqs + valid_seqs + test_seqs)\n",
    "\n",
    "all_seqs = list(set(all_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embed\n",
    "embeds = embed_seqs(model, model_deep, tokenizer, all_seqs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save with sql\n",
    "\n",
    "db_file = 'prot_vec_local.db'\n",
    "\n",
    "with sqlite3.connect(db_file) as conn:\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"CREATE TABLE IF NOT EXISTS embeddings (sequence TEXT PRIMARY KEY, embedding BLOB)\")\n",
    "    for seq, emb in tqdm(zip(all_seqs, embeds), total=len(all_seqs), desc='Saving to disk'):\n",
    "        emb_data = np.array(emb).tobytes()\n",
    "        c.execute(\"INSERT INTO embeddings VALUES (?, ?)\", (seq, emb_data))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
