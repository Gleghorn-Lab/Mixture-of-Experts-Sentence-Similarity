{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0879ec6943244e5bc1afbd812377848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91123b2bab774ecbab1f9c6a05ca87aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/570830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "482684"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('swiss_prot_vec.tsv', delimiter='\\t').rename(columns={\n",
    "    'EC number':'EC',\n",
    "    'Gene Ontology (molecular function)':'MF',\n",
    "    'Gene Ontology (biological process)':'BP',\n",
    "    'Gene Ontology (cellular component)':'CC',\n",
    "    'InterPro':'IP',\n",
    "    'Gene3D':'3D',\n",
    "    'Sequence':'seqs'\n",
    "}).astype('string')\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Combine the columns (excluding 'seqs') into a single string\n",
    "df['combined'] = df.progress_apply(lambda x: ' '.join(str(x[col]) for col in df.columns if col != 'seqs'), axis=1)\n",
    "\n",
    "df = df.sort_values(by='combined', key=lambda x: x.str.len(), ascending=False)\n",
    "\n",
    "# Drop duplicates based on the 'seqs' column, keeping the first occurrence (longest combined string)\n",
    "df = df.drop_duplicates(subset='seqs', keep='first')\n",
    "\n",
    "# Drop the temporary 'combined' column\n",
    "#df = df.drop('combined', axis=1)\n",
    "\n",
    "# Reset the index if needed\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(input, start=1, name='ec'):\n",
    "    id2label, label2id = {}, {}\n",
    "    for index, entry in enumerate(input, start=start):\n",
    "        entry = entry + '_' + name\n",
    "        id2label[index] = entry\n",
    "        label2id[entry] = index\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def process_descriptors(input_list, start=1, name='ec', filter_func=lambda d: d.strip()):\n",
    "    col_list, new_col = [], []\n",
    "    for item in tqdm(input_list, desc=f'{name} make dict'):\n",
    "        descriptors = str(item).split(';')\n",
    "        filtered_descriptors = [filter_func(d) for d in descriptors if filter_func(d) != 'None' and filter_func(d) != 'nan']\n",
    "        col_list.extend(filtered_descriptors)\n",
    "    col_list = sorted(list(set(col_list)))\n",
    "    col_list.pop(0)\n",
    "    id2label, label2id = create_dictionary(col_list, start=start, name=name)\n",
    "    \n",
    "    for item in tqdm(input_list, desc=f'{name} make new column'):\n",
    "        descriptors = str(item).split(';')\n",
    "        filtered_descriptors = [filter_func(d) for d in descriptors if filter_func(d) != 'None' and filter_func(d) != 'nan']\n",
    "        new_entry = [label2id.get(d+'_'+name, 0) for d in filtered_descriptors] or [0]\n",
    "        new_col.append(new_entry)\n",
    "    return new_col, id2label, label2id, len(col_list) + start\n",
    "\n",
    "\n",
    "def ec_processing(input_list, start=1, name='ec'):\n",
    "    return process_descriptors(input_list, start=start, name=name,\n",
    "                               filter_func=lambda d: d.strip() if '-' not in d and 'n' not in d else '')\n",
    "\n",
    "def go_processing(input_list, start=1, name='go'):\n",
    "    return process_descriptors(input_list, start=start, name=name,\n",
    "                               filter_func=lambda d: d[d.find('[GO:')+1:d.find(']')].strip())\n",
    "\n",
    "def cofactor_processing(input_list, start=1, name='co'):\n",
    "    return process_descriptors(input_list, start=start, name=name,\n",
    "                               filter_func=lambda d: d[d.find('Name=')+5:].strip() if 'Name' in d else '')\n",
    "\n",
    "def domain_processing(input_list, start=1, name='ip'):\n",
    "    return process_descriptors(input_list, start=start, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281bd8fb8ad34af8923ddd9c744ffdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ec make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54501850bf2440619d875ccf0cdd881f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ec make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e7bf0418e7493dac11d7e6d88d4e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mf make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0e990b746d49498b24f3dc3d2d196d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mf make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d451985b0940f0a4fb33cd4dfff070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bp make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d746bf56a1ae479eb02d2ba4bc293a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bp make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba9f570b0eb4dffa6e887b1eed9b0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cc make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f183d0832504ce5afec896ccf0e230b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cc make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014e054434b4971b84e1a284cdd4450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ip make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4632ff056bb943e5a9c27835bd799fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ip make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71ad765f99f431dbe575b9e6bbc792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3d make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a173ffd32fc44ed98817ca4dfa332d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3d make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d8e52576144b6189e2aa9e04f3145e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "co make dict:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0f55e0e6d14834aa34c43cae27c405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "co make new column:   0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EC\n",
    "ecs = df['EC'].tolist()\n",
    "new_ec_col, id2ec, ec2id, ec_len = ec_processing(ecs, name='ec', start=1)\n",
    "# MF\n",
    "mfs = df['MF'].tolist()\n",
    "new_mf_col, id2mf, mf2id, mf_len = go_processing(mfs, name='mf', start=ec_len+1)\n",
    "# BP\n",
    "bps = df['BP'].tolist()\n",
    "new_bp_col, id2bp, bp2id, bp_len = go_processing(bps, name='bp', start=mf_len+1)\n",
    "# CC\n",
    "ccs = df['CC'].tolist()\n",
    "new_cc_col, id2cc, cc2id, cc_len = go_processing(ccs, name='cc', start=bp_len+1)\n",
    "# IP\n",
    "ips = df['IP'].tolist()\n",
    "new_ip_col, id2ip, ip2id, ip_len = domain_processing(ips, name='ip', start=cc_len+1)\n",
    "# 3D\n",
    "threeds = df['3D'].tolist()\n",
    "new_threed_col, id2threed, threed2id, threed_len = domain_processing(threeds, name='3d', start=ip_len+1)\n",
    "# cofactor\n",
    "cos = df['Cofactor'].tolist()\n",
    "new_co_col, id2co, co2id, co_len = cofactor_processing(cos, name='co', start=threed_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73461\n",
      "73461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73461, 73461)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make full dicts and check for no duplicates\n",
    "all_id = [id2ec, id2mf, id2bp, id2cc, id2ip, id2threed, id2co]\n",
    "all_label = [ec2id, mf2id, bp2id, cc2id, ip2id, threed2id, co2id]\n",
    "\n",
    "id2label, label2id = {}, {}\n",
    "\n",
    "key_counts = 0\n",
    "for d in all_id:\n",
    "    key_counts += len(d.keys())\n",
    "    id2label.update(d)\n",
    "print(key_counts)\n",
    "\n",
    "key_counts = 0\n",
    "for d in all_label:\n",
    "    key_counts += len(d.keys())\n",
    "    label2id.update(d)\n",
    "print(key_counts)\n",
    "\n",
    "for k, v in id2label.items():\n",
    "    if k != label2id[v]:\n",
    "        print(v)\n",
    "\n",
    "for k, v in label2id.items():\n",
    "    if k != id2label[v]:\n",
    "        print(v)\n",
    "\n",
    "len(id2label.keys()), len(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [new_ec_col, new_mf_col, new_bp_col, new_cc_col, new_ip_col, new_threed_col, new_co_col]\n",
    "combined_list = [sorted([item for item in list(chain.from_iterable(element)) if item != 0]) for element in zip(*all_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482684,\n",
       "     Entry      EC Cofactor                                                 MF  \\\n",
       " 0  Q02248  [5487]      [0]  [10222, 10224, 5770, 11962, 12390, 13122, 8651...   \n",
       " 1  Q9WU82  [5487]      [0]  [10222, 10224, 5770, 11962, 12390, 13122, 8651...   \n",
       " 2  Q63673     [0]      [0]        [6887, 13312, 6902, 9954, 6690, 7005, 7026]   \n",
       " 3  Q62226     [0]      [0]        [6887, 13312, 6902, 9954, 6690, 7005, 7026]   \n",
       " 4  P04202  [5487]      [0]  [5809, 6699, 9662, 8651, 6935, 9878, 9986, 101...   \n",
       " \n",
       "                                                   BP  \\\n",
       " 0  [26917, 19952, 19951, 23152, 16547, 27100, 208...   \n",
       " 1  [26917, 19952, 19951, 23152, 16547, 27100, 208...   \n",
       " 2  [22764, 23379, 23243, 16023, 13897, 23242, 165...   \n",
       " 3  [22764, 23379, 23243, 16023, 13897, 23242, 165...   \n",
       " 4  [14373, 16512, 18904, 14687, 15390, 24738, 245...   \n",
       " \n",
       "                                                   CC  \\\n",
       " 0  [31929, 33069, 33229, 32175, 32176, 32174, 323...   \n",
       " 1  [31929, 33069, 33229, 32175, 32176, 32174, 323...   \n",
       " 2  [32331, 32114, 32332, 31839, 31845, 32408, 317...   \n",
       " 3  [32331, 32114, 32332, 31839, 31844, 31845, 324...   \n",
       " 4  [32331, 33614, 32114, 33392, 31801, 32408, 317...   \n",
       " \n",
       "                                                   IP                 3D  \\\n",
       " 0                    [43840, 46481, 34556, 44742, 0]         [69630, 0]   \n",
       " 1                    [43840, 46481, 34556, 44742, 0]         [69630, 0]   \n",
       " 2  [35749, 35843, 41798, 34632, 37291, 37292, 592...  [71079, 69835, 0]   \n",
       " 3  [35749, 35843, 41798, 34632, 37291, 37292, 592...  [71079, 69835, 0]   \n",
       " 4  [54566, 35895, 35292, 46713, 46291, 37574, 478...  [70372, 69775, 0]   \n",
       " \n",
       "                                                 seqs  \\\n",
       " 0  MATQADLMELDMAMEPDRKAAVSHWQQQSYLDSGIHSGATTTAPSL...   \n",
       " 1  MATQADLMELDMAMEPDRKAAVSHWQQQSYLDSGIHSGATTTAPSL...   \n",
       " 2  MLLLLARCFLVALASSLLVCPGLACGPGRGFGKRQHPKKLTPLAYK...   \n",
       " 3  MLLLLARCFLVILASSLLVCPGLACGPGRGFGKRRHPKKLTPLAYK...   \n",
       " 4  MPPSGLRLLPLLLPLPWLLVLTPGRPAAGLSTCKTIDMELVKRKRI...   \n",
       " \n",
       "                                             combined  \n",
       " 0  [5487, 5645, 5646, 5765, 5770, 5784, 8266, 865...  \n",
       " 1  [5487, 5645, 5646, 5765, 5770, 5784, 8266, 865...  \n",
       " 2  [6690, 6887, 6902, 7005, 7026, 9954, 13312, 13...  \n",
       " 3  [6690, 6887, 6902, 7005, 7026, 9954, 13312, 13...  \n",
       " 4  [5487, 5809, 6691, 6699, 6731, 6935, 8651, 954...  )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['EC'] = new_ec_col\n",
    "df['Cofactor'] = new_co_col\n",
    "df['MF'] = new_mf_col\n",
    "df['BP'] = new_bp_col\n",
    "df['CC'] = new_cc_col\n",
    "df['IP'] = new_ip_col\n",
    "df['3D'] = new_threed_col\n",
    "df['combined'] = combined_list\n",
    "df['string_combined'] = df['combined'].astype('string')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6935f4a37f81436fb712f2e99342cac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/482684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame length: 141387\n",
      "Final DataFrame head:\n",
      "    Entry      EC Cofactor                                                 MF  \\\n",
      "0  Q02248  [5487]      [0]  [10222, 10224, 5770, 11962, 12390, 13122, 8651...   \n",
      "1  Q9WU82  [5487]      [0]  [10222, 10224, 5770, 11962, 12390, 13122, 8651...   \n",
      "2  Q63673     [0]      [0]        [6887, 13312, 6902, 9954, 6690, 7005, 7026]   \n",
      "3  Q62226     [0]      [0]        [6887, 13312, 6902, 9954, 6690, 7005, 7026]   \n",
      "4  P04202  [5487]      [0]  [5809, 6699, 9662, 8651, 6935, 9878, 9986, 101...   \n",
      "\n",
      "                                                  BP  \\\n",
      "0  [26917, 19952, 19951, 23152, 16547, 27100, 208...   \n",
      "1  [26917, 19952, 19951, 23152, 16547, 27100, 208...   \n",
      "2  [22764, 23379, 23243, 16023, 13897, 23242, 165...   \n",
      "3  [22764, 23379, 23243, 16023, 13897, 23242, 165...   \n",
      "4  [14373, 16512, 18904, 14687, 15390, 24738, 245...   \n",
      "\n",
      "                                                  CC  \\\n",
      "0  [31929, 33069, 33229, 32175, 32176, 32174, 323...   \n",
      "1  [31929, 33069, 33229, 32175, 32176, 32174, 323...   \n",
      "2  [32331, 32114, 32332, 31839, 31845, 32408, 317...   \n",
      "3  [32331, 32114, 32332, 31839, 31844, 31845, 324...   \n",
      "4  [32331, 33614, 32114, 33392, 31801, 32408, 317...   \n",
      "\n",
      "                                                  IP                 3D  \\\n",
      "0                    [43840, 46481, 34556, 44742, 0]         [69630, 0]   \n",
      "1                    [43840, 46481, 34556, 44742, 0]         [69630, 0]   \n",
      "2  [35749, 35843, 41798, 34632, 37291, 37292, 592...  [71079, 69835, 0]   \n",
      "3  [35749, 35843, 41798, 34632, 37291, 37292, 592...  [71079, 69835, 0]   \n",
      "4  [54566, 35895, 35292, 46713, 46291, 37574, 478...  [70372, 69775, 0]   \n",
      "\n",
      "                                                seqs  \\\n",
      "0  MATQADLMELDMAMEPDRKAAVSHWQQQSYLDSGIHSGATTTAPSL...   \n",
      "1  MATQADLMELDMAMEPDRKAAVSHWQQQSYLDSGIHSGATTTAPSL...   \n",
      "2  MLLLLARCFLVALASSLLVCPGLACGPGRGFGKRQHPKKLTPLAYK...   \n",
      "3  MLLLLARCFLVILASSLLVCPGLACGPGRGFGKRRHPKKLTPLAYK...   \n",
      "4  MPPSGLRLLPLLLPLPWLLVLTPGRPAAGLSTCKTIDMELVKRKRI...   \n",
      "\n",
      "                                            combined  \n",
      "0  [5487, 5645, 5646, 5765, 5770, 5784, 8266, 865...  \n",
      "1  [5487, 5645, 5646, 5765, 5770, 5784, 8266, 865...  \n",
      "2  [6690, 6887, 6902, 7005, 7026, 9954, 13312, 13...  \n",
      "3  [6690, 6887, 6902, 7005, 7026, 9954, 13312, 13...  \n",
      "4  [5487, 5809, 6691, 6699, 6731, 6935, 8651, 954...  \n"
     ]
    }
   ],
   "source": [
    "unique_values = df['string_combined'].unique()\n",
    "\n",
    "# Create a dictionary to store the rows with the longest 'combined' string for each unique value\n",
    "unique_dict = {}\n",
    "\n",
    "# Iterate over the DataFrame and update the dictionary\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    combined_value = row['string_combined']\n",
    "    if combined_value not in unique_dict:\n",
    "        unique_dict[combined_value] = row\n",
    "    if combined_value == '[0]':\n",
    "        print(combined_value)\n",
    "\n",
    "# Create the final DataFrame from the dictionary values\n",
    "df_final = pd.DataFrame(unique_dict.values()).drop(columns=['string_combined'])\n",
    "\n",
    "print(\"Final DataFrame length:\", len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('processed_swiss_prot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_swiss_prot.csv', converters={\n",
    "    'EC': literal_eval,\n",
    "    'Cofactor': literal_eval,\n",
    "    'MF': literal_eval,\n",
    "    'BP': literal_eval,\n",
    "    'CC': literal_eval,\n",
    "    'IP': literal_eval,\n",
    "    '3D': literal_eval,\n",
    "    'combined': literal_eval\n",
    "})\n",
    "\n",
    "df['combined'] = df['combined'].apply(set)\n",
    "#df['EC'] = df['EC'].apply(set)\n",
    "#df['Cofactor'] = df['Cofactor'].apply(set)\n",
    "#df['MF'] = df['MF'].apply(set)\n",
    "#df['BP'] = df['BP'].apply(set)\n",
    "#df['CC'] = df['CC'].apply(set)\n",
    "#df['IP'] = df['IP'].apply(set)\n",
    "#df['3D'] = df['3D'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = df['combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21866ff48424a71a49e6aaed11e71d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(sets)\n",
    "all_indices = set(range(n))\n",
    "aspect_dict = defaultdict(set)\n",
    "\n",
    "for i, s in tqdm(enumerate(sets), total=n):\n",
    "    for item in s:\n",
    "        aspect_dict[item].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping = [set()] * n\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    seen = {j for item in sets[i] for j in aspect_dict[item]}\n",
    "    seen.remove(i)\n",
    "    overlapping[i] = seen\n",
    "\n",
    "non_overlapping = [all_indices - overlapping[i] - {i} for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baca6f9c2d8a43eca0674d6bf52d9f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141387 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "triplets = {\n",
    "    'EC': [],\n",
    "    'Cofactor': [],\n",
    "    'MF': [],\n",
    "    'BP': [],\n",
    "    'CC': [],\n",
    "    'IP': [],\n",
    "    '3D': []\n",
    "}\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    p = row['seqs']\n",
    "    \n",
    "    for aspect in triplets.keys():\n",
    "        item = random.choice(row[aspect])\n",
    "        if item != 0:\n",
    "            item_idxs = aspect_dict[item] - {i}\n",
    "            if len(item_idxs) > 0:\n",
    "                a_idx = random.choice(tuple(item_idxs))\n",
    "                a_item = df.loc[a_idx, 'seqs']\n",
    "                n_idx = random.choice(tuple(all_indices - item_idxs))\n",
    "                n_item = df.loc[n_idx, 'seqs']\n",
    "                triplets[aspect].append((p, a_item, n_item))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127204\n",
      "5161\n",
      "140410\n",
      "140952\n",
      "141335\n",
      "105121\n",
      "94537\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(triplets.items()):\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, ans, ns, acs = [], [], [], []\n",
    "for i, (k, v) in enumerate(triplets.items()):\n",
    "    for trip in v:\n",
    "        p, a, n = trip\n",
    "        ps.append(p)\n",
    "        ans.append(a)\n",
    "        ns.append(n)\n",
    "        acs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({\n",
    "    'positives':ps,\n",
    "    'anchors':ans,\n",
    "    'negatives':ns,\n",
    "    'aspects':acs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3742a95d2bdb42129efc88a94f2fff77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50121c9c65ac495e8c8f2cbeeb6088b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e812883a46448b4b10541330c35aa42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error while uploading 'data/epoch2-00001-of-00003.parquet' to the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/58/15/5815ee16998909f82705f1d41fb163c71c47bddceb6d872bbc7b21c38b4bd373/360c0e623b00f84eef323a3a2b2c5935b42271e92b70a083f7a36892d73c4170?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQFN2FTF47%2F20240319%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240319T155126Z&X-Amz-Expires=86400&X-Amz-Signature=015a8f4887b2056966c62051c4f71a6715dae49eb1be32be0b7001a57f6e9664&X-Amz-SignedHeaders=host&partNumber=25&uploadId=Ejsl9iOVFLvfcLhiXcwBBPAf8pwWM1ojvVNIO7r2_8o7TFIdl9ELeE4AEe3SYSIGNsITRXhmrUvluoigQg2vCbVxfXDVZ4GzGIFceGhyvsjK4wzS6vfIAOp7OxJB0DhB&x-id=UploadPart",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:400\u001b[0m, in \u001b[0;36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001b[1;34m(batch_action)\u001b[0m\n\u001b[0;32m    399\u001b[0m     operation \u001b[38;5;241m=\u001b[39m oid2addop[batch_action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m--> 400\u001b[0m     \u001b[43mlfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlfs_batch_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\lfs.py:228\u001b[0m, in \u001b[0;36mlfs_upload\u001b[1;34m(operation, lfs_batch_action, token)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m         )\n\u001b[1;32m--> 228\u001b[0m     \u001b[43m_upload_multi_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_action\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\lfs.py:325\u001b[0m, in \u001b[0;36m_upload_multi_part\u001b[1;34m(operation, header, chunk_size, upload_url)\u001b[0m\n\u001b[0;32m    320\u001b[0m     use_hf_transfer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    322\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m     _upload_parts_hf_transfer(operation\u001b[38;5;241m=\u001b[39moperation, sorted_parts_urls\u001b[38;5;241m=\u001b[39msorted_parts_urls, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_hf_transfer\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_upload_parts_iteratively\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# 3. Send completion request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\lfs.py:385\u001b[0m, in \u001b[0;36m_upload_parts_iteratively\u001b[1;34m(operation, sorted_parts_urls, chunk_size)\u001b[0m\n\u001b[0;32m    382\u001b[0m part_upload_res \u001b[38;5;241m=\u001b[39m http_backoff(\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, part_upload_url, data\u001b[38;5;241m=\u001b[39mfileobj_slice, retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m503\u001b[39m)\n\u001b[0;32m    384\u001b[0m )\n\u001b[1;32m--> 385\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart_upload_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m headers\u001b[38;5;241m.\u001b[39mappend(part_upload_res\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:329\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    326\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: \n\nBad request:",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlhallee/triplets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:5406\u001b[0m, in \u001b[0;36mDataset.push_to_hub\u001b[1;34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, branch, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[0;32m   5403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_dir:\n\u001b[0;32m   5404\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m config_name \u001b[38;5;28;01mif\u001b[39;00m config_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m-> 5406\u001b[0m additions, uploaded_size, dataset_nbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5409\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5415\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5418\u001b[0m \u001b[38;5;66;03m# Check if the repo already has a README.md and/or a dataset_infos.json to update them with the new split info (size and pattern)\u001b[39;00m\n\u001b[0;32m   5419\u001b[0m \u001b[38;5;66;03m# and delete old split shards (if they exist)\u001b[39;00m\n\u001b[0;32m   5420\u001b[0m repo_with_dataset_card, repo_with_dataset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:5235\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[1;34m(self, repo_id, data_dir, split, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[0;32m   5233\u001b[0m     uploaded_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   5234\u001b[0m     shard_addition \u001b[38;5;241m=\u001b[39m CommitOperationAdd(path_in_repo\u001b[38;5;241m=\u001b[39mshard_path_in_repo, path_or_fileobj\u001b[38;5;241m=\u001b[39mbuffer)\n\u001b[1;32m-> 5235\u001b[0m     \u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5238\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mshard_addition\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5244\u001b[0m     additions\u001b[38;5;241m.\u001b[39mappend(shard_addition)\n\u001b[0;32m   5246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m additions, uploaded_size, dataset_nbytes\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\utils\\hub.py:46\u001b[0m, in \u001b[0;36mpreupload_lfs_files\u001b[1;34m(hf_api, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreupload_lfs_files\u001b[39m(hf_api: HfApi, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mhf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4058\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[1;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[0;32m   4052\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   4053\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipped upload for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions_to_upload)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS file(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ignored by gitignore file).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4055\u001b[0m     )\n\u001b[0;32m   4057\u001b[0m \u001b[38;5;66;03m# Upload new LFS files\u001b[39;00m\n\u001b[1;32m-> 4058\u001b[0m \u001b[43m_upload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4059\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_lfs_additions_to_upload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4065\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If `create_pr`, we don't want to check user permission on the revision as users with read permission\u001b[39;49;00m\n\u001b[0;32m   4066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# should still be able to create PRs even if they don't have write permission on the target branch of the\u001b[39;49;00m\n\u001b[0;32m   4067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# PR (i.e. `revision`).\u001b[39;49;00m\n\u001b[0;32m   4068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4070\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m addition \u001b[38;5;129;01min\u001b[39;00m new_lfs_additions_to_upload:\n\u001b[0;32m   4071\u001b[0m     addition\u001b[38;5;241m.\u001b[39m_is_uploaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:410\u001b[0m, in \u001b[0;36m_upload_lfs_files\u001b[1;34m(additions, repo_type, repo_id, token, endpoint, num_threads, revision)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_actions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading 1 LFS file to the Hub\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 410\u001b[0m     \u001b[43m_wrapped_lfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_actions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS files to the Hub using up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_threads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m threads concurrently\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:402\u001b[0m, in \u001b[0;36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001b[1;34m(batch_action)\u001b[0m\n\u001b[0;32m    400\u001b[0m     lfs_upload(operation\u001b[38;5;241m=\u001b[39moperation, lfs_batch_action\u001b[38;5;241m=\u001b[39mbatch_action, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while uploading \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;241m.\u001b[39mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error while uploading 'data/epoch2-00001-of-00003.parquet' to the Hub."
     ]
    }
   ],
   "source": [
    "data.push_to_hub('lhallee/triplets', split='epoch2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MATQADLMELDMAMEPDRKAAVSHWQQQSYLDSGIHSGATTTAPSLSGKGNPEEEDVDTSQVLYEWEQGFSQSFTQEQVADIDGQYAMTRAQRVRAAMFPETLDEGMQIPSTQFDAAHPTNVQRLAEPSQMLKHAVVNLINYQDDAELATRAIPELTKLLNDEDQVVVNKAAVMVHQLSKKEASRHAIMRSPQMVSAIVRTMQNTNDVETARCTAGTLHNLSHHREGLLAIFKSGGIPALVKMLGSPVDSVLFYAITTLHNLLLHQEGAKMAVRLAGGLQKMVALLNKTNVKFLAITTDCLQILAYGNQESKLIILASGGPQALVNIMRTYTYEKLLWTTSRVLKVLSVCSSNKPAIVEAGGMQALGLHLTDPSQRLVQNCLWTLRNLSDAATKQEGMEGLLGTLVQLLGSDDINVVTCAAGILSNLTCNNYKNKMMVCQVGGIEALVRTVLRAGDREDITEPAICALRHLTSRHQEAEMAQNAVRLHYGLPVVVKLLHPPSHWPLIKATVGLIRNLALCPANHAPLREQGAIPRLVQLLVRAHQDTQRRTSMGGTQQQFVEGVRMEEIVEGCTGALHILARDVHNRIVIRGLNTIPLFVQLLYSPIENIQRVAAGVLCELAQDKEAAEAIEAEGATAPLTELLHSRNEGVATYAAAVLFRMSEDKPQDYKKRLSVELTSSLFRTEPMAWNETADLGLDIGAQGEALGYRQDDPSYRSFHSGGYGQDALGMDPMMEHEMGGHHPGADYPVDGLPDLGHAQDLMDGLPPGDSNQLAWFDTDL',\n",
       " 'MATALPRTLGELQLYRILQKANLLSYFDAFIQQGGDDVQQLCEAGEEEFLEIMALVGMASKPLHVRRLQKALRDWVTNPGLFNQPLTSLPVSSIPIYKLPEGSPTWLGISCNSYERSSSAREPHLKVPKCAATTCVQSLGQGKSEVGSLALQSVSESRLWQGHHTTESEHSLSPADLGSPASPKESSEALDAAAALSVAECVERMAPTLPKSDLNEVKELLKNNKKLAKMIGHIFEMSDEDPHKEEEIRKYSAIYGRFDSKRKDGKHLTLHELTVNEAAAQLCVKDNALLTRRDELFALARQVSREVTYKYTYRTTRLKCGERDELSPKRIKMEDGFPDFQESVPTLFQQARAKSEELAGLSSQAEKGMAKQMELLCAQAGYERLQQERRLMAGLYRQSSGEQSPDGGLPSDGSDGQGERPLNLRIPSVQNRQPHHFVVDGELSRLYSNEVKSHSSESLGILKDYPHSAFTLEKKVIKTEPEDSR',\n",
       " 'MTIRIPSGEEADYTLHLPRILCLHGGGTNARIFRMQCRVLERFLRSTFRFVYAEAPFAAQPGSDVTSVYKDHGPFKAWLRCTAADPDRSAQEVVKKINLSIATAMYDDDMRGATGEWIALLGFSQGAKVAASILYAQQTIQQRLGERAATRPRFRFAVLMAGRGPLVWLLPETSSGPGSIPMGLVDAASPSMLDSEPELPTDSTEHMLRLPTLHVHGLRDPGLSLHRRLLRSYCQSDSVSLVEWEGEHRVPLKTKDVTAVVDQIYALARDTGVLDSWC')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets['EC'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('triplets_epoch_1.pkl', 'wb') as f:\n",
    "    pickle.dump(triplets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For looking / documenting swiss prot duplicates\n",
    "seqs = df['seqs'].tolist()\n",
    "entry_ids = df['Entry'].tolist()\n",
    "print(len(seqs), len(list(set(seqs))))\n",
    "\n",
    "counts = {}\n",
    "duplicates = {}\n",
    "\n",
    "for i, seq in enumerate(seqs):\n",
    "    if seq in counts:\n",
    "        counts[seq] += 1\n",
    "        duplicates[seq].append(entry_ids[i])\n",
    "    else:\n",
    "        counts[seq] = 1\n",
    "        duplicates[seq] = [entry_ids[i]]\n",
    "\n",
    "# Write duplicates to a text file\n",
    "with open(\"swiss_prot_duplicates.txt\", \"w\") as file:\n",
    "    for seq, ids in duplicates.items():\n",
    "        if len(ids) > 1:\n",
    "            count = len(ids)\n",
    "            file.write(f\"{count}\\t{' '.join(ids)}\\n\")\n",
    "            file.write(f\"{seq}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
