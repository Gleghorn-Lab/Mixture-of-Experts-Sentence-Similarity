{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict, Counter\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset, DatasetDict\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390da68b007546fcaa00ccd350acd91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, delimiter='\\t').rename(columns={\n",
    "        'EC number':'EC',\n",
    "        'Gene Ontology (molecular function)':'MF',\n",
    "        'Gene Ontology (biological process)':'BP',\n",
    "        'Gene Ontology (cellular component)':'CC',\n",
    "        'Sequence':'seqs'\n",
    "    }).astype('string')\n",
    "    print(len(df))\n",
    "    df['combined'] = df.progress_apply(lambda x: ' '.join(str(x[col]) for col in df.columns if col != 'seqs'), axis=1)\n",
    "    df = df.sort_values(by='combined', key=lambda x: x.str.len(), ascending=False)\n",
    "    df = df.drop_duplicates(subset='seqs', keep='first')\n",
    "    df = df.drop('combined', axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_dictionary(input, start=1, name='ec'):\n",
    "    id2label, label2id = {}, {}\n",
    "    for index, entry in enumerate(input, start=start):\n",
    "        entry = entry + '_' + name\n",
    "        id2label[index] = entry\n",
    "        label2id[entry] = index\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def process_descriptors(input_list,\n",
    "                        start=1,\n",
    "                        name='ec',\n",
    "                        id2label=None,\n",
    "                        label2id=None,\n",
    "                        filter_func=lambda d: d.strip()):\n",
    "    col_list, new_col = [], []\n",
    "\n",
    "    if id2label == None or label2id == None:\n",
    "        for item in tqdm(input_list, desc=f'{name} make dicts'):\n",
    "            descriptors = str(item).split(';')\n",
    "            filtered_descriptors = [filter_func(d) for d in descriptors]\n",
    "            filtered_descriptors = [d for d in filtered_descriptors if d and d.lower() != 'none' and d.lower() != 'nan']\n",
    "            col_list.extend(filtered_descriptors)\n",
    "        col_list = sorted(list(set(col_list)))\n",
    "        if '' in col_list:\n",
    "            col_list.remove('')\n",
    "        len_col_list = len(col_list)\n",
    "        id2label, label2id = create_dictionary(col_list, start=start, name=name)\n",
    "    else:\n",
    "        len_col_list = len(id2label.keys())\n",
    "\n",
    "    for item in tqdm(input_list, desc=f'{name} make new column'):\n",
    "        descriptors = str(item).split(';')\n",
    "        filtered_descriptors = [filter_func(d) for d in descriptors]\n",
    "        filtered_descriptors = [d for d in filtered_descriptors if d and d.lower() != 'none' and d.lower() != 'nan']\n",
    "        new_entry = [label2id[d + '_' + name] for d in filtered_descriptors if d] or [0]\n",
    "        new_col.append(new_entry)\n",
    "\n",
    "    return new_col, id2label, label2id, len_col_list + start\n",
    "\n",
    "\n",
    "def ec_processing(input_list, start=1, name='ec', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id,\n",
    "                               filter_func=lambda d: d.strip() if '-' not in d and 'n' not in d else '')\n",
    "\n",
    "\n",
    "def go_processing(input_list, start=1, name='go', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id,\n",
    "                               filter_func=lambda d: d[d.find('[GO:')+1:d.find(']')].strip())\n",
    "\n",
    "\n",
    "def cofactor_processing(input_list, start=1, name='co', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id,\n",
    "                               filter_func=lambda d: d[d.find('Name=')+5:].strip() if 'Name' in d else '')\n",
    "\n",
    "\n",
    "def domain_processing(input_list, start=1, name='ip', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id)\n",
    "\n",
    "\n",
    "def replace_df(df, all_cols):\n",
    "    new_ec_col, new_mf_col, new_bp_col, new_cc_col = all_cols\n",
    "    combined_list = [sorted([item for item in list(chain.from_iterable(element)) if item != 0])\n",
    "                     for element in zip(*all_cols) if element != [0]]\n",
    "\n",
    "    df['EC'] = new_ec_col\n",
    "    df['MF'] = new_mf_col\n",
    "    df['BP'] = new_bp_col\n",
    "    df['CC'] = new_cc_col\n",
    "    df['combined'] = combined_list\n",
    "    df['string_combined'] = df['combined'].astype('string')\n",
    "\n",
    "    unique_dict = {}\n",
    "\n",
    "    # Iterate over the DataFrame and update the dictionary\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        combined_value = row['string_combined']\n",
    "        if combined_value not in unique_dict:\n",
    "            unique_dict[combined_value] = row\n",
    "        if combined_value == '[0]':\n",
    "            print(combined_value)\n",
    "\n",
    "    # Create the final DataFrame from the dictionary values\n",
    "    df_final = pd.DataFrame(unique_dict.values()).drop(columns=['string_combined'])\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "df = load_df('doubles.tsv')\n",
    "# EC\n",
    "ecs = df['EC'].tolist()\n",
    "new_ec_col, id2ec, ec2id, ec_len = ec_processing(ecs, name='ec', start=1)\n",
    "# MF\n",
    "mfs = df['MF'].tolist()\n",
    "new_mf_col, id2mf, mf2id, mf_len = go_processing(mfs, name='mf', start=ec_len+1)\n",
    "# BP\n",
    "bps = df['BP'].tolist()\n",
    "new_bp_col, id2bp, bp2id, bp_len = go_processing(bps, name='bp', start=mf_len+1)\n",
    "# CC\n",
    "ccs = df['CC'].tolist()\n",
    "new_cc_col, id2cc, cc2id, cc_len = go_processing(ccs, name='cc', start=bp_len+1)\n",
    "\n",
    "# make full dicts and check for no duplicates\n",
    "all_id = [id2ec, id2mf, id2bp, id2cc]\n",
    "all_label = [ec2id, mf2id, bp2id, cc2id]\n",
    "\n",
    "id2label, label2id = {}, {}\n",
    "\n",
    "key_counts = 0\n",
    "for d in all_id:\n",
    "    key_counts += len(d.keys())\n",
    "    id2label.update(d)\n",
    "print(key_counts)\n",
    "\n",
    "key_counts = 0\n",
    "for d in all_label:\n",
    "    key_counts += len(d.keys())\n",
    "    label2id.update(d)\n",
    "print(key_counts)\n",
    "\n",
    "for k, v in id2label.items():\n",
    "    if k != label2id[v]:\n",
    "        print(v)\n",
    "\n",
    "for k, v in label2id.items():\n",
    "    if k != id2label[v]:\n",
    "        print(v)\n",
    "\n",
    "print(len(id2label.keys()), len(label2id.keys()))\n",
    "\n",
    "all_cols = [new_ec_col, new_mf_col, new_bp_col, new_cc_col]\n",
    "\n",
    "df_final = replace_df(df, all_cols)\n",
    "\n",
    "print(len(df_final))\n",
    "\n",
    "df_final.to_csv('processed_doubles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "df = load_df('trembl_all_aspects_raw.tsv')\n",
    "# EC\n",
    "ecs = df['EC'].tolist()\n",
    "new_ec_col, id2ec, ec2id, ec_len = ec_processing(ecs, name='ec', start=1)\n",
    "# MF\n",
    "mfs = df['MF'].tolist()\n",
    "new_mf_col, id2mf, mf2id, mf_len = go_processing(mfs, name='mf', start=ec_len+1)\n",
    "# BP\n",
    "bps = df['BP'].tolist()\n",
    "new_bp_col, id2bp, bp2id, bp_len = go_processing(bps, name='bp', start=mf_len+1)\n",
    "# CC\n",
    "ccs = df['CC'].tolist()\n",
    "new_cc_col, id2cc, cc2id, cc_len = go_processing(ccs, name='cc', start=bp_len+1)\n",
    "# IP\n",
    "ips = df['IP'].tolist()\n",
    "new_ip_col, id2ip, ip2id, ip_len = domain_processing(ips, name='ip', start=cc_len+1)\n",
    "# 3D\n",
    "threeds = df['3D'].tolist()\n",
    "new_threed_col, id2threed, threed2id, threed_len = domain_processing(threeds, name='3d', start=ip_len+1)\n",
    "# cofactor\n",
    "cos = df['Cofactor'].tolist()\n",
    "new_co_col, id2co, co2id, co_len = cofactor_processing(cos, name='co', start=threed_len+1)\n",
    "\n",
    "\n",
    "all_cols = [new_ec_col, new_mf_col, new_bp_col, new_cc_col, new_ip_col, new_threed_col, new_co_col]\n",
    "df = replace_df(df, all_cols)\n",
    "\n",
    "# remove dups\n",
    "for col in df.columns:\n",
    "    mask = df[col].apply(lambda x: x != [0]\n",
    "                        and (not isinstance(x, list) or len(x) > 0)\n",
    "                        and (not isinstance(x, str) or x.strip() != '[]'))\n",
    "    df = df.loc[mask]\n",
    "\n",
    "df_filtered = df[~df['seqs'].isin(df_final['seqs'])]\n",
    "\n",
    "print(len(df_filtered))\n",
    "df_filtered.to_csv('processed_trembl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading previously processed\n",
    "df = pd.read_csv('processed_doubles.csv', converters={\n",
    "    'EC': literal_eval,\n",
    "    'MF': literal_eval,\n",
    "    'BP': literal_eval,\n",
    "    'CC': literal_eval,\n",
    "    'combined': literal_eval\n",
    "})\n",
    "\n",
    "df['combined'] = df['combined'].apply(set)\n",
    "\n",
    "ec_aspect = df['EC'].apply(set).tolist()\n",
    "mf_aspect = df['MF'].apply(set).tolist()\n",
    "bp_aspect = df['BP'].apply(set).tolist()\n",
    "cc_aspect = df['CC'].apply(set).tolist()\n",
    "\n",
    "single_aspect_dict = {\n",
    "    'EC': ec_aspect,\n",
    "    'MF': mf_aspect,\n",
    "    'BP': bp_aspect,\n",
    "    'CC': cc_aspect,\n",
    "}\n",
    "\n",
    "print(df.head(1))\n",
    "\n",
    "sets = df['combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pairs(seqs, indices):\n",
    "    A, B = [], []\n",
    "    for i, j in indices:\n",
    "        A.append(seqs[i])\n",
    "        B.append(seqs[j])\n",
    "    return A, B\n",
    "\n",
    "\n",
    "def flatten_and_count(tuples_list):\n",
    "    flattened_list = [item for tuple_item in tuples_list for item in tuple_item]\n",
    "    element_counts = Counter(flattened_list)\n",
    "    most_repeated_element, count = element_counts.most_common(1)[0]\n",
    "    return most_repeated_element, count\n",
    "\n",
    "\n",
    "def make_pairs_nonredundant(pairs, max_count=10):\n",
    "    index_count = defaultdict(int)\n",
    "    seen_pairs = set()\n",
    "    nonredundant_pairs = []\n",
    "    sorted_pairs = sorted(pairs, key=lambda x: index_count[x[0]] + index_count[x[1]])\n",
    "    for pair in tqdm(sorted_pairs, desc='Making non redundant'):\n",
    "        i, j = pair\n",
    "        if (i, j) not in seen_pairs and (j, i) not in seen_pairs:\n",
    "            if index_count[i] < max_count and index_count[j] < max_count:\n",
    "                nonredundant_pairs.append(pair)\n",
    "                seen_pairs.add((i, j))\n",
    "                index_count[i] += 1\n",
    "                index_count[j] += 1\n",
    "    return nonredundant_pairs\n",
    "\n",
    "\n",
    "def calculate_similar_sets(sets):\n",
    "    pairs_10, pairs_30, pairs_50, pairs_70 = [], [], [], []\n",
    "    count_10, count_30, count_50, count_70 = 0, 0, 0, 0\n",
    "    len_sets = len(sets)\n",
    "    set_range = set(list(range(len_sets)))\n",
    "    for i in tqdm(set_range, desc='Measuring set similarity'):\n",
    "        set_i = sets[i]\n",
    "        new_range = set_range - {i}\n",
    "        for j in new_range:\n",
    "            set_j = sets[j]\n",
    "            intersection = set_i & set_j\n",
    "            union = set_i.union(set_j)\n",
    "            similarity = len(intersection) / len(union)\n",
    "            if similarity >= 0.1:\n",
    "                count_10 += 1\n",
    "                pairs_10.append((i, j))\n",
    "            if similarity >= 0.3:\n",
    "                count_30 += 1\n",
    "                pairs_30.append((i, j))\n",
    "            if similarity >= 0.5:\n",
    "                count_50 += 1\n",
    "                pairs_50.append((i, j))\n",
    "            if similarity >= 0.7:\n",
    "                count_70 += 1\n",
    "                pairs_70.append((i, j))\n",
    "    pairs = (pairs_10, pairs_30, pairs_50, pairs_70)\n",
    "    counts = (count_10, count_30, count_50, count_70)\n",
    "    return pairs, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, counts = calculate_similar_sets(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percents')\n",
    "for count in counts:\n",
    "    print(f'{count * 100 / (len(sets) ** 2)}%')\n",
    "print('Redundant pair length')\n",
    "for pair in pairs:\n",
    "    print(len(pair) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_10 = make_pairs_nonredundant(pairs[0])\n",
    "non_30 = make_pairs_nonredundant(pairs[1])\n",
    "non_50 = make_pairs_nonredundant(pairs[2])\n",
    "non_70 = make_pairs_nonredundant(pairs[3])\n",
    "len(non_10), len(non_30), len(non_50), len(non_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = df['seqs'].tolist()\n",
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a10, b10 = retrieve_pairs(seqs, non_10)\n",
    "a30, b30 = retrieve_pairs(seqs, non_30)\n",
    "a50, b50 = retrieve_pairs(seqs, non_50)\n",
    "a70, b70 = retrieve_pairs(seqs, non_70)\n",
    "\n",
    "data = DatasetDict({\n",
    "    '10': Dataset.from_dict({\n",
    "        'a': a10,\n",
    "        'b': b10\n",
    "    }),\n",
    "    '30': Dataset.from_dict({\n",
    "        'a': a30,\n",
    "        'b': b30\n",
    "    }),\n",
    "    '50': Dataset.from_dict({\n",
    "        'a': a50,\n",
    "        'b': b50\n",
    "    }),\n",
    "    '70': Dataset.from_dict({\n",
    "        'a': a70,\n",
    "        'b': b70\n",
    "    })\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({\n",
    "    'a':a50,\n",
    "    'b':b50\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.push_to_hub('lhallee/ProteinDoublesAll', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = data.train_test_split(test_size=5000, seed=42)\n",
    "test_valid = split['test'].train_test_split(test_size=2500)\n",
    "valid_set = test_valid['train']\n",
    "test_set = test_valid['test']\n",
    "print(f\"Validation set size: {valid_set.num_rows}\")\n",
    "print(f\"Test set size: {test_set.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dataset = DatasetDict({\n",
    "    'train':split['train'],\n",
    "    'valid':valid_set,\n",
    "    'test':test_set\n",
    "})\n",
    "double_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dataset.push_to_hub('lhallee/ProteinDouble50', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/lhallee/ProteinDoublesAll (Request ID: Root=1-6627e3d4-67c59ff320b27a9363addab3;3c0a1816-10d2-4e84-aa38-1aa9ceaff56b)\n\nInternal Error - We're working hard to fix this as soon as possible!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m full_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlhallee/ProteinDoublesAll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m full_data\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:2587\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2582\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2583\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2584\u001b[0m )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2587\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:2259\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2257\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   2258\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 2259\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1910\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1905\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m   1906\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1907\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1908\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1909\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1910\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1912\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1913\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1914\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1855\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[0;32m   1851\u001b[0m             msg\n\u001b[0;32m   1852\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If the dataset is private or gated, make sure to log in with `huggingface-cli login` or visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to ask for access.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1853\u001b[0m         )\n\u001b[0;32m   1854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1855\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [sibling\u001b[38;5;241m.\u001b[39mrfilename \u001b[38;5;28;01mfor\u001b[39;00m sibling \u001b[38;5;129;01min\u001b[39;00m dataset_info\u001b[38;5;241m.\u001b[39msiblings]:  \u001b[38;5;66;03m# contains a dataset script\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m     fs \u001b[38;5;241m=\u001b[39m HfFileSystem(endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT, token\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mtoken)\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:1828\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1826\u001b[0m hf_api \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1828\u001b[0m     dataset_info \u001b[38;5;241m=\u001b[39m \u001b[43mhf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# noqa catch any exception of hf_hub and consider that the dataset doesn't exist\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1836\u001b[0m         e,\n\u001b[0;32m   1837\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m         ),\n\u001b[0;32m   1842\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2291\u001b[0m, in \u001b[0;36mHfApi.dataset_info\u001b[1;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m-> 2291\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2292\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/lhallee/ProteinDoublesAll (Request ID: Root=1-6627e3d4-67c59ff320b27a9363addab3;3c0a1816-10d2-4e84-aa38-1aa9ceaff56b)\n\nInternal Error - We're working hard to fix this as soon as possible!"
     ]
    }
   ],
   "source": [
    "full_data = load_dataset('lhallee/ProteinDoublesAll')\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
