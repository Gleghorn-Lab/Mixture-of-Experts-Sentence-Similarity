general_args:
  model_path: ''
  ESM: false # ESM or BERT
  model_type: 'ProteinVec'
  contact_head: false
  huggingface_username: lhallee

# MOE
  single_moe: false
  moe_type: 'topk'
  MOE: false
  token_moe: false
  num_experts: 8  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 7  # Number of tasks for MulitTask learning
  expert_loss: false
  MI: false
  wBAL: !!float 0.05
  wEX: !!float 0.01

# Model settings
  hidden_dropout_prob: 0.0  # Dropout rate for the model
  attention_probs_dropout_prob: 0.0

# New tokens
  new_special_tokens: false  # Add new special tokens for each domain token seeded with CLS
  domains: # List of domain tags
    - '[EC]'
    - '[BP]'
    - '[CC]'
    - '[MF]'
    - '[IP]'
    - '[3D]'

# data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/triplets'
  a_col: 'anchors'  # First feature column name in datasets # doubles as anchor
  b_col: 'seqs'  # Second feature column name in datasets
  p_col: 'positives'
  n_col: 'negatives'
  label_col: 'aspects'
  max_length: 512  # Maximum length of the sequences
  num_labels: 2
  valid_size: 2500
  test_size: 2500

# wandb settings
  wandb: false
  wandb_project: SSPR
  wandb_name: protein_vec_eval

# Paths
  log_path: './results_protein_vec_hf_down.txt'  # Path to save the log file
  weight_path: 'lhallee/ProteinVec'  # Path to the model weights to load
  save_path: 'provec.pt'

# Training settings
  patience: 100

# Eval settings
  limits: false  # Lets user define limits for F1max

# Training arguments for HF trainer
training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 4
  per_device_eval_batch_size: !!int 4
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_steps: !!int 100
  save_strategy: !!str steps
  save_steps: !!int 2500
  save_total_limit: !!int 3
  evaluation_strategy: !!str steps
  eval_steps: 2500
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: true
  fp16_full_eval: true
  seed: !!int 42
  eval_accumulation_steps: 500
  group_by_length: false
  length_column_name: !!str length
  metric_for_best_model: !!str f1_max

# Training arugments for HF trainer during evaluation
eval_training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 64
  per_device_eval_batch_size: !!int 64
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-4
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 200
  warmup_steps: !!int 100
  save_strategy: !!str epoch
  save_total_limit: !!int 3
  evaluation_strategy: !!str epoch
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: false
  seed: !!int 7
  metric_for_best_model: !!str loss

# General eval args during evaluation
eval_args:

# Paths
  plm_path: ''
  data_paths:
    - 'lhallee/dl_binary_reg'
#    - 'lhallee/EC_reg'
#    - 'lhallee/CC_reg'
#    - 'lhallee/MF_reg'
#    - 'lhallee/BP_reg'
#    - 'lhallee/dl_ten_reg'
#    - 'lhallee/MetalIonBinding_reg'
  weight_path: ''
  log_path: './results_protein_vec_hf_down.txt'
  db_path: prot_vec_hf.db

# New tokens
  new_special_tokens: false
  domains:
    - '[CC]'
    - '[EC]'
    - '[CC]'
    - '[MF]'
    - '[BP]'
    - '[CC]'
    - '[IP]'

# Model settings
  input_dim: 512
  hidden_dim: 8096
  dropout: 0.1
  num_layers: 2
  nhead: 8
  num_labels: 4

# Data settings
  trim: true
  max_length: 1024
  skip: false # skip embedding, already embedded
  read_scaler: 100
  full: false
  new_special_tokens: true
  sql: false

# Trainging settings
  patience: 10
