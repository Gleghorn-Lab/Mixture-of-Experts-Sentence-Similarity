{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, EvalPrediction\n",
    "from models.modeling_moebert import MoEBertForSentenceSimilarity, BertForSentenceSimilarity\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss, confusion_matrix\n",
    "from models.load_model import MoEBertLoadWeights\n",
    "from data_zoo import *\n",
    "from utils import get_yaml, log_metrics, load_model\n",
    "from trainer import HF_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.losses import clip_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.842775687217713"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = []\n",
    "for _ in range(1000):\n",
    "    a = torch.rand(8, 768)\n",
    "    b = torch.rand(8, 768)\n",
    "    losses.append(clip_loss(a, b).item())\n",
    "\n",
    "sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def data_collator(features):\n",
    "    batch = {key: torch.stack([f[key] for f in features]) for key in features[0]}\n",
    "    return batch\n",
    "\n",
    "\n",
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, a, b, c_labels, r_labels, tokenizer, domains, add_tokens, max_length=512):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c_labels = c_labels\n",
    "        self.r_labels = r_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.domains = domains\n",
    "        self.max_length = max_length\n",
    "        self.add_tokens = add_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.a)\n",
    "\n",
    "    def __getitem__(self, idx): # Maybe need a version for non MOE\n",
    "        r_label = torch.tensor(self.r_labels[idx], dtype=torch.long)\n",
    "        c_label = torch.tensor(self.c_labels[idx], dtype=torch.float)\n",
    "        tokenized_a = self.tokenizer(self.a[idx],\n",
    "                                     return_tensors='pt',\n",
    "                                     padding='max_length',\n",
    "                                     truncation=True,\n",
    "                                     max_length=self.max_length)\n",
    "        tokenized_b = self.tokenizer(self.b[idx],\n",
    "                                     return_tensors='pt',\n",
    "                                     padding='max_length',\n",
    "                                     truncation=True,\n",
    "                                     max_length=self.max_length)\n",
    "        if self.add_tokens:\n",
    "            domain_token = self.tokenizer(self.domains[int(r_label.item())],\n",
    "                                          add_special_tokens=False).input_ids[0]  # get the domain token\n",
    "            tokenized_a['input_ids'][0][0] = domain_token  # replace the cls token with the domain token\n",
    "            tokenized_b['input_ids'][0][0] = domain_token  # replace the cls token with the domain token\n",
    "        return {\n",
    "            'input_ids_a': tokenized_a['input_ids'].squeeze(),\n",
    "            'attention_mask_a': tokenized_a['attention_mask'].squeeze(),\n",
    "            'input_ids_b': tokenized_b['input_ids'].squeeze(),\n",
    "            'attention_mask_b': tokenized_b['attention_mask'].squeeze(),\n",
    "            'labels': c_label,\n",
    "            'r_labels': r_label\n",
    "        }\n",
    "\n",
    "\n",
    "def get_datasets_train(args, tokenizer):\n",
    "    data_paths = args['data_paths']\n",
    "    domains = args['domains']\n",
    "    add_tokens = args['new_special_tokens']\n",
    "    max_length = args['max_length']\n",
    "    a_col = args['a_col']\n",
    "    b_col = args['b_col']\n",
    "    label_col = args['label_col']\n",
    "\n",
    "    train_a, train_b, train_c_label, train_r_label = [], [], [], []\n",
    "    valid_a, valid_b, valid_c_label, valid_r_label = [], [], [], []\n",
    "    test_a, test_b, test_c_label, test_r_label = [], [], [], []\n",
    "    for i, data_path in enumerate(data_paths):\n",
    "        dataset = load_dataset(data_path)\n",
    "        train = dataset['train']\n",
    "        valid = dataset['valid']\n",
    "        test = dataset['test']\n",
    "        train_a.extend(train[a_col])\n",
    "        train_b.extend(train[b_col])\n",
    "        train_c_label.extend(train[label_col])\n",
    "        train_r_label.extend([i] * len(train[label_col]))\n",
    "        valid_a.extend(valid[a_col])\n",
    "        valid_b.extend(valid[b_col])\n",
    "        valid_c_label.extend(valid[label_col])\n",
    "        valid_r_label.extend([i] * len(valid[label_col]))\n",
    "        test_a.extend(test[a_col])\n",
    "        test_b.extend(test[b_col])\n",
    "        test_c_label.extend(test[label_col])\n",
    "        test_r_label.extend([i] * len(test[label_col]))\n",
    "    train_dataset = TextDataset(train_a, train_b, train_c_label, train_r_label,\n",
    "                                tokenizer, domains, add_tokens, max_length)\n",
    "    valid_dataset = TextDataset(valid_a, valid_b, valid_c_label, valid_r_label,\n",
    "                                tokenizer, domains, add_tokens,  max_length)\n",
    "    test_dataset = TextDataset(test_a, test_b, test_c_label, test_r_label,\n",
    "                               tokenizer, domains, add_tokens,  max_length)\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "yargs = get_yaml('yamls/SE/copd.yaml')\n",
    "args = yargs['general_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSentenceSimilarity(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(args)\n",
    "model = model.to(device).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = get_datasets_train(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] chronic obstructive pulmonary disease ( copd ) is a systemic disease., several long non - coding rnas ( lncrnas ) have been identified to be aberrantly expressed in copd patients., this study investigated the role of lncrna cancer susceptibility candidate 2 ( casc2 ) in copd, as well as its potential mechanism., fifty smokers with copd and another 50 smokers without copd were recruited., receiver operating characteristic curve was constructed to assess the diagnostic value of casc2 in copd patients., 16hbe cells were treated with cigarette smoke extract ( cse ) to establish a cell model. qrt - pcr was used for the measurement of mrna levels., the cell viability and apoptosis were detected by using cell counting kit - 8 and flow cytometry assay., enzyme - linked immunosorbent assay was performed to detect the levels of proinflammatory cytokines., luciferase reporter assay was performed for the target gene analysis., serum casc2 was dramatically decreased in copd patients compared with smokers without copd, and was positively associated with fev1 ( forced expiratory volume in one second )., serum casc2 was overexpressed in severe copd patients, and had the diagnostic accuracy to distinguish copd patients from smokers., casc2 overexpression alleviated cse - induced apoptosis and inflammation in 16hbe cells., casc2 functions as a cerna of mir - 18a - 5p., upregulation of mir - 18a - 5p reversed the influence of casc2 on cell apoptosis and inflammation in 16hbe cells., igf1 was the target gene of mir - 18a - 5p., casc2 was downregulated in copd patients and it might be a promising biomarker for the disease diagnosis., overexpression of casc2 might inhibit the bronchial epithelial cell apoptosis and inflammation via targeting mir - 18a - 5p / igf1 axis., the reviews of this paper are available via the supplemental material section. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] traffic - related air pollution particulate matter 2. 5 ( trapm2. 5 ), is involved in chronic obstructive pulmonary disease ( copd ), which is characterized by airway inflammation., specifically, these harmful particles or gases can increase chronic airway inflammation., some recent studies have shown that lncrnas are closely related to copd and participate in the regulation of airway inflammation., however, the precise mechanisms remain unknown., in the present study, we investigated the effect of trapm2. 5 on airway inflammation in human bronchial epithelial cells ( hbecs ) and the underlying mechanisms mediated by a lncrna., after exposure to trapm2. 5, the novel lncrna rp11 - 86h7. 1 was markedly upregulated in hbecs., functional assays indicated that the lncrna rp11 - 86h7. 1 was required for the trapm2. 5 - induced expression of inflammatory factors in hbecs., a mechanistic study demonstrated that lncrna rp11 - 86h7. 1 might participate in trapm2. 5 - induced inflammatory responses by activating the nf - κb signaling pathway., moreover, the lncrna rp11 - 86h7. 1 can promote the inflammatory response by acting as a competing endogenous rna of mir - 9 - 5p, reversing the inhibitory effect of its target gene nfkb1, and sustaining nf - κb activation., in summary, our study elucidates the pro - inflammatory roles of the lncrna rp11 - 86h7. 1 - mir - 9 - 5p - nfkb1 regulatory network in airway inflammation induced by trapm2. 5 and indicates that the components of this network might serve as novel diagnostic biomarkers and potential therapeutic targets. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for k, v in train_dataset[0].items():\n",
    "    if 'input' in k:\n",
    "        print(tokenizer.decode(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [{k:v.to(device) for k,v in train_dataset[i].items()} for i in range(4)]\n",
    "batch = data_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask_a'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6094, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
