general_args:
  model_path: 'allenai/scibert_scivocab_uncased'

# MOE
  moe_type: 'topk'
  single_moe: true
  MOE: true 
  token_moe: true
  num_experts: 4  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 7  # Number of tasks for MulitTask learning
  contrastive_loss: clip
  expert_loss: true
  MI: false  wBAL: !!float 0.01
  wEX: !!float 0.05

# Model settings
  hidden_dropout_prob: 0.05  # Dropout rate for the model
  attention_probs_dropout_prob: 0.0

# New tokens
  new_special_tokens: true  # Add new special tokens for each domain token seeded with CLS
  domains: # List of domain tags
    - '[EC]'
    - '[CO]'
    - '[BP]'
    - '[CC]'
    - '[MF]'
    - '[IP]'
    - '[3D]'

#data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/triplets'
  a_col: 'anchors'  # First feature column name in datasets # doubles as anchor
  b_col: 'seqs'  # Second feature column name in datasets
  p_col: 'positives'
  n_col: 'negatives'
  label_col: 'aspects'
  max_length: 512  # Maximum length of the sequences
  num_labels: 2
  valid_size: 100
  test_size: 100

# wandb settings
  wandb: false
  wandb_project: SSPR
  wandb_name: triplet_test

# Paths
  log_path: './results.txt'  # Path to save the log file
  weight_path: null  # Path to the model weights to load

# Training settings
  patience: 10

# Eval settings
  limits: false  # Lets user define limits for F1max

# Training arguments for HF trainer
training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 2
  per_device_eval_batch_size: !!int 2
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_ratio: !!float 0.0
  warmup_steps: !!int 500
  save_strategy: !!str steps
  save_steps: !!int 2500
  save_total_limit: !!int 3
  evaluation_strategy: !!str steps
  eval_steps: 10
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: false
  seed: !!int 42
  eval_accumulation_steps: null
  group_by_length: false
  length_column_name: !!str length
  metric_for_best_model: !!str loss

# Training arugments for HF trainer during evaluation
eval_training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 64
  per_device_eval_batch_size: !!int 64
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-4
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_steps: !!int 100
  save_strategy: !!str epoch
  save_total_limit: !!int 3
  evaluation_strategy: !!str epoch
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: true
  seed: !!int 7
  metric_for_best_model: !!str f1_max
