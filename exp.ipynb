{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run local_prot_vec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py --yaml_path yamls/protein_vec.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def are_models_equal(model1, model2):\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "    \n",
    "    if len(state_dict1) != len(state_dict2):\n",
    "        print('Length')\n",
    "        return False\n",
    "    \n",
    "    for key in state_dict1:\n",
    "        if key not in state_dict2:\n",
    "            print('Key')\n",
    "            return False\n",
    "        \n",
    "        if not torch.equal(state_dict1[key], state_dict2[key]):\n",
    "            print('Equal')\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "if are_models_equal(disk_model.cpu(), hf_model.cpu()):\n",
    "    print(\"The models are equal.\")\n",
    "else:\n",
    "    print(\"The models are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.load_data import get_datasets_test_triplet\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_paths': ['lhallee/triplets'],\n",
    "    'domains': ['[EC]', '[CC]', '[MF]', '[BP]', '[CC]', '[CC]', '[IP]'],\n",
    "    'new_special_tokens': False,\n",
    "    'max_length':512, \n",
    "    'p_col': 'positives',\n",
    "    'a_col': 'anchors',\n",
    "    'n_col': 'negatives',\n",
    "    'label_col': 'aspects',\n",
    "    'model_type': 'ProteinVec'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('lhallee/ProteinVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_datasets = get_datasets_test_triplet(args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = triplet_datasets[0]\n",
    "ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.isnan(torch.tensor(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.modeling_moesm import EsmExpert\n",
    "\n",
    "\n",
    "class SentenceEnforcedSwitchMoeBlock(nn.Module): ### Test\n",
    "    def __init__(self, config, expert):\n",
    "        \"\"\"\n",
    "        Sentence level MoE, single expert chosen\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.num_experts = config.num_experts\n",
    "        self.experts = nn.ModuleList([expert(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_labels: torch.tensor) -> torch.Tensor:\n",
    "        # (batch, seq_len, hidden_size), (batch,) -> from 0 to num_experts-1\n",
    "        sorted_indices = torch.argsort(router_labels) # sort in order of expert idx\n",
    "        hidden_states = hidden_states[sorted_indices] # apply sort\n",
    "        router_labels = router_labels[sorted_indices] # apply sort\n",
    "        expert_idxs = torch.unique(router_labels) # find all experts needed\n",
    "        grouped_hidden_states = torch.split(hidden_states, tuple(torch.bincount(router_labels))) # split sorted hidden_states\n",
    "\n",
    "        expert_outputs = []\n",
    "        for idx, group in zip(expert_idxs, grouped_hidden_states):\n",
    "            expert_output = self.experts[idx](group) # sne batched groups to their experts\n",
    "            expert_outputs.append(expert_output)\n",
    "\n",
    "        concatenated_outputs = torch.cat(expert_outputs, dim=0)\n",
    "        final_hidden_states = concatenated_outputs[torch.argsort(sorted_indices)] # put back to original order\n",
    "        return final_hidden_states  # (batch, sequence_length, hidden_dim)\n",
    "    \n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 128\n",
    "        self.num_experts = 4\n",
    "        self.intermediate_size = 256\n",
    "        self.hidden_dropout_prob = 0.0\n",
    "\n",
    "\n",
    "config = Config()\n",
    "moe_block = SentenceEnforcedSwitchMoeBlock(config, EsmExpert)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random tensors for testing\n",
    "batch_size = 8\n",
    "seq_length = 16\n",
    "hidden_size = config.hidden_size\n",
    "\n",
    "hidden_states = torch.randn(batch_size, seq_length, hidden_size)\n",
    "router_labels = torch.randint(0, config.num_experts, (batch_size,))\n",
    "\n",
    "# Test the forward pass of the SentenceEnforcedSwitchMoeBlock\n",
    "output = moe_block(hidden_states, router_labels)\n",
    "\n",
    "# Print the shapes of input and output tensors\n",
    "print(\"Input hidden states shape:\", hidden_states.shape)\n",
    "print(\"Router labels shape:\", router_labels.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_labels = torch.tensor([2, 0, 1, 2, 1])\n",
    "print(router_labels)\n",
    "sorted_indices = torch.argsort(router_labels)\n",
    "\n",
    "sorted_labels = router_labels[sorted_indices]\n",
    "\n",
    "unsorted_indices = torch.argsort(sorted_indices)\n",
    "\n",
    "router_labels = sorted_labels[unsorted_indices]\n",
    "print(router_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights differ\n",
      "All weights match\n",
      "Model loaded in  0.11 minutes\n",
      "734.2 million total parameters\n",
      "534.8 million effective parameters\n",
      "Approximately 2.74 GB of memory in fp32\n",
      "\n",
      "MoEsmVec(\n",
      "  (base): T5EncoderModel(\n",
      "    (shared): Embedding(144, 768)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(144, 768)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (relative_attention_bias): Embedding(64, 12)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wi_1): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-47): 47 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wi_1): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (base_adapter): BaseAdapter(\n",
      "    (base_gate): Linear(in_features=768, out_features=48, bias=True)\n",
      "    (base_conv): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (base_proj): Linear(in_features=768, out_features=480, bias=True)\n",
      "    (combine_conv): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (esm_adapter): EsmAdapter(\n",
      "    (base_gate): Linear(in_features=768, out_features=48, bias=True)\n",
      "    (esm_gate): Linear(in_features=480, out_features=12, bias=True)\n",
      "    (base_conv): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (esm_conv): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (proj): Sequential(\n",
      "    (0): Linear(in_features=1248, out_features=1248, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1248, out_features=768, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (esm): MoEsmModel(\n",
      "    (embeddings): MoEsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
      "    )\n",
      "    (encoder): MoEsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x MoEsmLayer(\n",
      "          (attention): MoEsmAttention(\n",
      "            (self): MoEsmSelfAttention(\n",
      "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): MoEsmSelfOutput(\n",
      "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (dropout): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm()\n",
      "          )\n",
      "          (LayerNorm): LayerNorm()\n",
      "          (moe_block): TokenTopKMoeBlock(\n",
      "            (gate): Linear(in_features=480, out_features=8, bias=False)\n",
      "            (experts): ModuleList(\n",
      "              (0-7): 8 x EsmExpert(\n",
      "                (intermediate_up): Linear(in_features=480, out_features=1920, bias=True)\n",
      "                (intermediate_down): Linear(in_features=1920, out_features=480, bias=True)\n",
      "                (dropout): Dropout(p=0.05, inplace=False)\n",
      "                (new_linear): Linear(in_features=480, out_features=1920, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (aux_loss): LoadBalancingLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import get_yaml\n",
    "from models.load_model import load_models\n",
    "\n",
    "def calc_memory(model, inputs, device, name='Layer'):\n",
    "    model.to(device)\n",
    "    try:\n",
    "        inputs = inputs.to(device)\n",
    "        memory_before = torch.cuda.memory_allocated(device)\n",
    "        output = model(inputs)        \n",
    "    except:\n",
    "        inputs = (input.to(device) for input in inputs)\n",
    "        memory_before = torch.cuda.memory_allocated(device)\n",
    "        output = model(*inputs)\n",
    "\n",
    "    memory_after = torch.cuda.memory_allocated(device)\n",
    "    vram_usage = memory_after - memory_before\n",
    "    print(f\"Process {name} VRAM usage: {vram_usage / 1024 / 1024:.2f} MB\")\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class arguments:\n",
    "    yaml_path = 'yamls/MOE/moesm_double_35.yaml'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "args = arguments()\n",
    "yargs = get_yaml(args.yaml_path)\n",
    "for key, value in yargs['general_args'].items(): # copy yaml config into args\n",
    "    setattr(args, key, value)\n",
    "for key, value in yargs['training_args'].items():\n",
    "    setattr(args, key, value)\n",
    "\n",
    "\n",
    "args.gated = True\n",
    "args.moe_type = 'topk'\n",
    "args.token_moe = True\n",
    "args.topk = 2\n",
    "args.num_experts = 8\n",
    "\n",
    "model, tokenizer = load_models(args) # if eval and skip, not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = model.base\n",
    "esm = model.esm\n",
    "base_adapter = model.base_adapter\n",
    "esm_adapter = model.esm_adapter\n",
    "proj = model.proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_length = 1024\n",
    "vocab_size = 20  # IDs from 0 to 20\n",
    "\n",
    "base_input = torch.randint(0, 20, (batch_size, max_length))\n",
    "esm_input = torch.randint(0, 20, (batch_size, max_length))\n",
    "base_adapter_input = (torch.rand(batch_size, 49, max_length, 768), torch.rand(batch_size, max_length, 480))\n",
    "esm_adapter_input = (torch.rand(batch_size, 49, max_length, 768), torch.rand(batch_size, 13, max_length, 480))\n",
    "proj_input = torch.rand(batch_size, 768 + 480)\n",
    "\n",
    "names = ['ANKH', 'ESM', 'BASE ADAPT', 'ESM ADAPT', 'PROJ']\n",
    "models = [base, esm, base_adapter, esm_adapter, proj]\n",
    "inputs = [base_input, esm_input, base_adapter_input, esm_adapter_input, proj_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ANKH VRAM usage: 11.12 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ESM VRAM usage: 2544.31 MB\n",
      "Process BASE ADAPT VRAM usage: 47.97 MB\n",
      "Process ESM ADAPT VRAM usage: 58.50 MB\n",
      "Process PROJ VRAM usage: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "for model, input, name in zip(models, inputs, names):\n",
    "    calc_memory(model, input, device, name)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
