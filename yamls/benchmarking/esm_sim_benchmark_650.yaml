general_args:
  model_path: 'facebook/esm2_t33_650M_UR50D'
  ESM: true # ESM or BERT
  model_type: 'Triplet'
  contact_head: false
  huggingface_username: lhallee
  token: ''

# MOE
  moe_type: 'topk'
  single_moe: false
  MOE: false 
  token_moe: false
  num_experts: 4  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 7  # Number of tasks for MulitTask learning
  contrastive_loss: clip
  MI_loss: true
  wBAL: !!float 0.01
  wMI: !!float 0.05

# Model settings
  hidden_dropout_prob: 0.05  # Dropout rate for the model
  attention_probs_dropout_prob: 0.0

# New tokens
  new_special_tokens: true  # Add new special tokens for each domain token seeded with CLS
  domains: # List of domain tags
    - '[EC]'
    - '[CO]'
    - '[BP]'
    - '[CC]'
    - '[MF]'
    - '[IP]'
    - '[3D]'

#data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/triplets'
  a_col: 'anchors'  # First feature column name in datasets # doubles as anchor
  b_col: 'seqs'  # Second feature column name in datasets
  p_col: 'positives'
  n_col: 'negatives'
  label_col: 'aspects'
  max_length: 512  # Maximum length of the sequences
  num_labels: 2
  valid_size: 100
  test_size: 100

# wandb settings
  wandb: false
  wandb_project: SSPR
  wandb_name: triplet_test

# Paths
  log_path: './sim_benchmark_650.txt'  # Path to save the log file
  weight_path: null  # Path to the model weights to load
  save_path: 'test.pt'

# Training settings
  patience: 10

# Eval settings
  limits: false  # Lets user define limits for F1max

# Training arguments for HF trainer
training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 2
  per_device_eval_batch_size: !!int 2
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_ratio: !!float 0.0
  warmup_steps: !!int 500
  save_strategy: !!str steps
  save_steps: !!int 2500
  save_total_limit: !!int 3
  evaluation_strategy: !!str steps
  eval_steps: 10
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: false
  seed: !!int 42
  eval_accumulation_steps: null
  group_by_length: false
  length_column_name: !!str length
  metric_for_best_model: !!str loss

# Training arugments for HF trainer during evaluation
eval_training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  per_device_train_batch_size: !!int 64
  per_device_eval_batch_size: !!int 64
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-4
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  warmup_steps: !!int 100
  save_strategy: !!str epoch
  save_total_limit: !!int 3
  evaluation_strategy: !!str epoch
  logging_strategy: steps
  logging_steps: !!int 100
  fp16: true
  seed: !!int 7
  metric_for_best_model: !!str f1_max

# General eval args during evaluation
eval_args:

# Paths
  plm_path: facebook/esm2_t6_8M_UR50D
  data_paths:
    - 'lhallee/EC_reg'
    - 'lhallee/CC_reg'
    - 'lhallee/MF_reg'
    - 'lhallee/BP_reg'
    - 'lhallee/dl_binary_reg'
    - 'lhallee/dl_ten_reg'
    - 'lhallee/MetalIonBinding_reg'
  weight_path: ''
  log_path: ./results.txt
  db_path: embeddings.db

# New tokens
  domains:
    - '[EC]'
    - '[CC]'
    - '[MF]'
    - '[BP]'
    - '[CC]'
    - '[CC]'
    - '[IP]'

# Model settings
  input_dim: 768
  hidden_dim: 768
  intermediate_dim: 2048
  dropout: 0.1
  num_layers: 2
  nhead: 8

# Data settings
  trim: false
  max_length: 512
  skip: false # skip embedding, already embedded

# Trainging settings
  patience: 1
