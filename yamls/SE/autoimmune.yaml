general_args:
  model_path: 'allenai/scibert_scivocab_uncased'
  ESM: false # ESM or BERT
  model_type: 'SentenceSimilarity'

  moe_type: 'topk'
  MOE: false 
  token_moe: false

  contact_head: false

  num_experts: 8  # Number of experts
  topk: 2  # Number of experts per block
  num_tasks: 5  # Number of tasks for MulitTask learning
  hidden_dropout_prob: 0.05  # Dropout rate for the model
  contrastive_loss: clip
  MI_loss: false

  add_during_eval: false # add tokens during eval
  new_special_tokens: false  # Add new special tokens for each domain token seeded with CLS
  domains:  # List of domain tags
    - '[IMM]'
  
  wBAL: !!float 0.1
  wMI: !!float 0.1

#data_settings:
  data_paths:  # Paths to the datasets
    - 'lhallee/abstract_domain_autoimmune'
  a_col: 'a'  # First feature column name in datasets
  b_col: 'b'  # Second feature column name in datasets
  label_col: 'label'  # Label column name in datasets
  max_length: 512  # Maximum length of the sequences

#wandb settings
  wandb: true
  wandb_project: SSPR
  wandb_name: immune_mixed_full

#misc:
  project_name: 'SciMOE'  # Name of the project
  log_path: './results.txt'  # Path to save the log file
  weight_path: 'autoimmune_single.pt'  # Path to the model weights to load

  patience: 3
  limits: false  # Lets user define limits for F1max


training_args:
  output_dir: !!str ./output
  logging_dir: !!str ./logs
  report_to: null

  per_device_train_batch_size: !!int 20
  per_device_eval_batch_size: !!int 20
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 1e-5
  lr_scheduler_type: !!str cosine
  weight_decay: !!float 0.01

  num_train_epochs: !!int 10
  warmup_ratio: !!float 0.0
  warmup_steps: !!int 500

  save_strategy: !!str steps
  save_steps: !!int 5000
  save_total_limit: !!int 3

  evaluation_strategy: !!str steps
  eval_steps: 5000

  logging_strategy: steps
  logging_steps: !!int 100

  bf16: false
  fp16: true

  seed: !!int 42

  eval_accumulation_steps: null
  group_by_length: false
  length_column_name: !!str length
  save_safetensors: true
  metric_for_best_model: !!str f1_max
