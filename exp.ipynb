{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7291666865348816, 0.5378358364105225)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_max(pred, target):\n",
    "    \"\"\"\n",
    "        F1 score with the optimal threshold, adapted from TorchDrug.\n",
    "\n",
    "        This function first enumerates all possible thresholds for deciding positive and negative\n",
    "        samples, and then pick the threshold with the maximal F1 score.\n",
    "\n",
    "        Parameters:\n",
    "            pred (Tensor): predictions of shape :math:`(B, N)`\n",
    "            target (Tensor): binary targets of shape :math:`(B, N)`\n",
    "      \"\"\"\n",
    "\n",
    "    order = pred.argsort(descending=True, dim=1)\n",
    "    target = target.gather(1, order)\n",
    "    precision = target.cumsum(1) / torch.ones_like(target).cumsum(1)\n",
    "    recall = target.cumsum(1) / (target.sum(1, keepdim=True) + 1e-10)\n",
    "    is_start = torch.zeros_like(target).bool()\n",
    "    is_start[:, 0] = 1\n",
    "    is_start = torch.scatter(is_start, 1, order, is_start)\n",
    "\n",
    "    all_order = pred.flatten().argsort(descending=True)\n",
    "    order = order + torch.arange(order.shape[0], device=order.device).unsqueeze(1) * order.shape[1]\n",
    "    order = order.flatten()\n",
    "    inv_order = torch.zeros_like(order)\n",
    "    inv_order[order] = torch.arange(order.shape[0], device=order.device)\n",
    "    is_start = is_start.flatten()[all_order]\n",
    "    all_order = inv_order[all_order]\n",
    "    precision = precision.flatten()\n",
    "    recall = recall.flatten()\n",
    "    all_precision = precision[all_order] - \\\n",
    "                    torch.where(is_start, torch.zeros_like(precision), precision[all_order - 1])\n",
    "    all_precision = all_precision.cumsum(0) / is_start.cumsum(0)\n",
    "    all_recall = recall[all_order] - \\\n",
    "                torch.where(is_start, torch.zeros_like(recall), recall[all_order - 1])\n",
    "    all_recall = all_recall.cumsum(0) / pred.shape[0]\n",
    "    all_f1 = 2 * all_precision * all_recall / (all_precision + all_recall + 1e-10)\n",
    "    max_index = all_f1.argmax()\n",
    "    max_f1 = all_f1[max_index]\n",
    "    max_threshold = pred.flatten()[max_index]\n",
    "    return max_f1.item(), max_threshold.item() # outputs f1max and threshold to get max f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMoELoadWeights:\n",
    "    def __init__(self, args):\n",
    "        self.model_path = args.model_path\n",
    "        self.model_type = args.model_type\n",
    "        self.num_local_experts = args.num_local_experts\n",
    "        self.num_experts_per_tok = args.num_experts_per_tok\n",
    "        self.use_router_loss = args.use_router_loss\n",
    "        self.output_hidden_states = args.output_hidden_states\n",
    "        self.num_labels = args.num_labels\n",
    "        self.Bert_base = None\n",
    "        self.config = None\n",
    "\n",
    "    def get_seeded_model(self):\n",
    "        start_time = time.time()\n",
    "        if self.model_type == 'Model':\n",
    "            from transformers import BertModel\n",
    "            self.Bert_base = BertModel.from_pretrained(self.model_path)\n",
    "            self.config = self.get_config(self.Bert_base)\n",
    "            model = BertModel(config=self.config)\n",
    "        elif self.model_type == 'MaskedLM':\n",
    "            from transformers import BertForMaskedLM\n",
    "            self.Bert_base = BertModel.from_pretrained(self.model_path)\n",
    "            self.config = self.get_config(self.Bert_base)\n",
    "            model = BertForMaskedLM(config=self.config)\n",
    "        elif self.model_type == 'SequenceClassification':\n",
    "            from transformers import BertForSequenceClassification\n",
    "            self.Bert_base = BertModel.from_pretrained(self.model_path, num_labels=self.num_labels)\n",
    "            self.config = self.get_config(self.Bert_base)\n",
    "            model = BertForSequenceClassification(config=self.config)\n",
    "        elif self.model_type == 'TokenClassification':\n",
    "            from transformers import BertForTokenClassification\n",
    "            self.Bert_base = BertModel.from_pretrained(self.model_path, num_labels=self.num_labels)\n",
    "            self.config = self.get_config(self.Bert_base)\n",
    "            model = BertForTokenClassification(config=self.config)\n",
    "        else: print(f'You entered {self.model_type}\\nValid options are:\\nModel , MaskedLM , SequenceClassification , TokenClassification')\n",
    "        model = self.match_weights(model)\n",
    "        end_time = time.time()\n",
    "        print('Model loaded in ', round((end_time - start_time) / 60, 2), 'minutes')\n",
    "        total, effective, mem = self.count_parameters(model)\n",
    "        print(f'{total} million total parameters')\n",
    "        print(f'{effective} million effective parameters')\n",
    "        print(f'Approximately {mem} GB of memory in fp32\\n')\n",
    "        return model\n",
    "\n",
    "    def get_config(self, model):\n",
    "        config = model.config\n",
    "        config.num_local_experts = self.num_local_experts\n",
    "        config.num_experts_per_tok = self.num_experts_per_tok\n",
    "        config.use_router_loss = self.use_router_loss\n",
    "        config.output_router_logits = self.use_router_loss\n",
    "        config.output_hidden_states = self.output_hidden_states\n",
    "        return config\n",
    "\n",
    "    def check_for_match(self, model): # Test for matching parameters\n",
    "        all_weights_match = True\n",
    "        for name, param in self.Bert_base.named_parameters(): # for shared parameters\n",
    "            if name in model.state_dict():\n",
    "                pre_trained_weight = param.data\n",
    "                moe_weight = model.state_dict()[name].data\n",
    "                if not torch.equal(pre_trained_weight, moe_weight):\n",
    "                    all_weights_match = False\n",
    "                    break\n",
    "    \n",
    "        for i in range(self.config.num_hidden_layers): # for experts\n",
    "            for j in range(self.config.num_local_experts):\n",
    "                moe_encoder_layer = model.Bert.encoder.layer[i] if self.model_type != 'Model' else model.encoder.layer[i]\n",
    "                Bert_encoder_layer = self.Bert_base.Bert.encoder.layer[i] if self.model_type != 'Model' else self.Bert_base.encoder.layer[i] \n",
    "                if not torch.equal(moe_encoder_layer.moe_block.experts[j].intermediate_up.weight,\n",
    "                                Bert_encoder_layer.intermediate.dense.weight):\n",
    "                    all_weights_match = False\n",
    "                if not torch.equal(moe_encoder_layer.moe_block.experts[j].intermediate_down.weight,\n",
    "                                Bert_encoder_layer.output.dense.weight):\n",
    "                    all_weights_match = False\n",
    "\n",
    "        if all_weights_match:\n",
    "            print('All weights match')\n",
    "        else:\n",
    "            print('Some weights differ')\n",
    "\n",
    "    def match_weights(self, model): # Seeds MoBert experts with linear layers of Bert\n",
    "        self.check_for_match(model)\n",
    "        for name1, param1 in self.Bert_base.named_parameters():\n",
    "            for name2, param2 in model.named_parameters():\n",
    "                if name1 == name2:\n",
    "                    model.state_dict()[name2].data.copy_(param1.data)\n",
    "\n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            for j in range(self.config.num_local_experts):\n",
    "                moe_encoder_layer = model.Bert.encoder.layer[i] if self.model_type != 'Model' else model.encoder.layer[i]\n",
    "                Bert_encoder_layer = self.Bert_base.Bert.encoder.layer[i] if self.model_type != 'Model' else self.Bert_base.encoder.layer[i] \n",
    "                moe_encoder_layer.moe_block.experts[j].intermediate_up = copy.deepcopy(Bert_encoder_layer.intermediate.dense)\n",
    "                moe_encoder_layer.moe_block.experts[j].intermediate_down = copy.deepcopy(Bert_encoder_layer.output.dense)\n",
    "        self.check_for_match(model)\n",
    "        return model\n",
    "\n",
    "    def count_parameters_in_layer(self, layer):\n",
    "        \"\"\"Counts parameters in a regular layer.\"\"\"\n",
    "        return sum(p.numel() for p in layer.parameters())\n",
    "\n",
    "    def count_parameters(self, model):\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        non_effective_params = 0\n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            for j in range(self.config.num_local_experts - self.config.num_experts_per_tok):\n",
    "                moe_encoder_layer = model.encoder.layer[i] if self.model_type == 'Model' else model.Bert.encoder.layer[i]\n",
    "                non_effective_params += self.count_parameters_in_layer(moe_encoder_layer.moe_block.experts[j].intermediate_up)\n",
    "                non_effective_params += self.count_parameters_in_layer(moe_encoder_layer.moe_block.experts[j].intermediate_down)\n",
    "        effective_params = total_params - non_effective_params\n",
    "        memory_bytes = total_params * 4  # 4 bytes for 32-bit floats\n",
    "        memory_gig = round(memory_bytes / (1024 ** 3), 2)\n",
    "        return round(total_params / 1e6, 1), round(effective_params / 1e6, 1), memory_gig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertExpert(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Esm intermediate and output linear layers for MOE\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.intermediate_up = nn.Linear(config.hidden_size, config.intermediate_size) # EsmIntermediate dense\n",
    "        self.intermediate_down = nn.Linear(config.intermediate_size, config.hidden_size) # EsmOutput dense\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.intermediate_up(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.intermediate_down(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertMoeBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.num_experts = config.num_experts\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([BertExpert(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        router_output = self.gate(hidden_states) # (batch, sequence_length, n_experts)\n",
    "        router_logits = router_output.mean(dim=1) # (batch, n_experts)\n",
    "        router_choice = F.softmax(router_logits, dim=-1).argmax(dim=-1) # (batch)\n",
    "        final_hidden_states = torch.stack([self.experts[router_choice[i]](hidden_states[i]) for i in range(len(hidden_states))])\n",
    "        return final_hidden_states, router_logits # (batch, sequence_length, hidden_dim), (batch, num_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "tokenizer = BertTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertMoeBlock(\n",
       "  (gate): Linear(in_features=384, out_features=2, bias=False)\n",
       "  (experts): ModuleList(\n",
       "    (0-1): 2 x BertExpert(\n",
       "      (intermediate_up): Linear(in_features=384, out_features=1536, bias=True)\n",
       "      (intermediate_down): Linear(in_features=1536, out_features=384, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = model.config\n",
    "config.num_experts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router choice:  torch.Size([8]) tensor([1, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "ex = torch.rand(8, 16, 384)\n",
    "block = BertMoeBlock(config)\n",
    "test = block(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 384])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertIntermediate(\n",
       "  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "  (intermediate_act_fn): GELUActivation()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0].intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
