{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c2da84-2365-4e17-9c47-75eee1823524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_protein_moe import trans_basic_block, trans_basic_block_Config\n",
    "from utils_search import *\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import re\n",
    "import gc\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from huggingface_protein_vec import ProteinVec, ProteinVecConfig\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a600f00-c74b-4637-9c03-2acb3add515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Protein-Vec MOE model checkpoint and config\n",
    "vec_model_cpnt = 'protein_vec_models/protein_vec.ckpt'\n",
    "vec_model_config = 'protein_vec_models/protein_vec_params.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc0ff9-1315-4f11-a29a-da5fa7f1a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the ProtTrans model and ProtTrans tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"lhallee/prot_t5_enc\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"lhallee/prot_t5_enc\").to(device).eval()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771ef42-4c25-4715-b5bd-ce8d736e803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "vec_model_config = trans_basic_block_Config.from_json(vec_model_config)\n",
    "model_deep = trans_basic_block.load_from_checkpoint(vec_model_cpnt, config=vec_model_config).to(device).eval()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ce172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_seqs(model, model_deep, tokenizer, seqs, device):\n",
    "    sampled_keys = np.array(['TM', 'PFAM', 'GENE3D', 'ENZYME', 'MFO', 'BPO', 'CCO'])\n",
    "    all_cols = np.array(['TM', 'PFAM', 'GENE3D', 'ENZYME', 'MFO', 'BPO', 'CCO'])\n",
    "    masks = [all_cols[k] in sampled_keys for k in range(len(all_cols))]\n",
    "    masks = torch.logical_not(torch.tensor(masks, dtype=torch.bool))[None,:]\n",
    "\n",
    "    embed_all_sequences = []\n",
    "    for seq in tqdm(seqs): \n",
    "        protrans_sequence = featurize_prottrans([seq], model, tokenizer, device)\n",
    "        embedded_sequence = embed_vec(protrans_sequence, model_deep, masks, device)\n",
    "        embed_all_sequences.append(embedded_sequence)\n",
    "    return np.concatenate(embed_all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a00d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MRALKARSRLASRRQLKKLDEDSLTKQPEEVFDVLEKLGEGSYGSVYKAIHKETGQIVAIKQVPVESDLQEIIKEISIMQQCDSPHVVKYYGSYFKNTDLWIVMEYCGAGSVSDIIRLRNKTLTEDEIATILQSTLKGLEYLHFMRKIHRDIKAGNILLNTEGHAKLADFGVAGQLTDTMAKRNTVIGTPFWMAPEVIQEIGYNCVADIWSLGITAIEMAEGKPPYADIHPMRAIFMIPTNPPPTFRKPEVWSDNFMDFVKQCLVKSPEQRATATQLLQHPFVKSAKGAAILRDLINEAMDVKLKRQEAQQRAVDQDDDENSEEDEMDSGTMVRAAGDDMGTVRVASTMSGGANTMIEHGDTLPSQLGTMVINTEDEEEEGTMKRRDETMQPAKPSFLEYFEQKEKENQINSFGKNVSGSLKNSSDWKIPQDGDYEFLKSWTVEDLQKRLSALDPMMEQEMEEIRQKYRSKRQPILDAIEAKKRRQQNF',\n",
       " 'MSAPTADIRARAPEAKKVHIADTAINRHNWYKHVNWLNVFLIIGIPLYGCIQAFWVPLQLKTAIWAVIYYFFTGLGITAGYHRLWAHCSYSATLPLRIWLAAVGGGAVEGSIRWWARDHRAHHRYTDTDKDPYSVRKGLLYSHLGWMVMKQNPKRIGRTDISDLNEDPVVVWQHRNYLKVVFTMGLAVPMLVAGLGWGDWLGGFVYAGILRIFFVQQATFCVNSLAHWLGDQPFDDRNSPRDHVITALVTLGEGYHNFHHEFPSDYRNAIEWHQYDPTKWSIWAWKQLGLAYDLKKFRANEIEKGRVQQLQKKLDRKRATLDWGTPLDQLPVMEWDDYVEQAKNGRGLVAIAGVVHDVTDFIKDHPGGKAMISSGIGKDATAMFNGGVYYHSNAAHNLLSTMRVGVIRGGCEVEIWKRAQKENVEYVRDGSGQRVIRAGEQPTKIPEPIPTADAA',\n",
       " 'LCFQLLPVVIGAAVVAFTVLYLFFRSVSSYHKKRGKKSPVTLQDPSAKYALPLIDKKEINHDTKRFTFGLPSPDHVLGLPIGQHVYLSAKVNGNLVIRAYTPVSSDEDKGFVDLVVKVYYKNMHKDYPQGGKMSQYLDNMNIGDTVDFRGPNGLLVYKGKGKFEIRPDKKSEAKVKHIKNLGMIAGGTGITPMLQLIHQITADPEDKTKCYLLFANQSEKDILLRAELEDVARNHADQIKLWYTLDKAPQGWKYGTGFINAEMIKNHLPPPANDVLVLMCGPLPMIQCACQPNLTKLGYKMEDMFTY',\n",
       " 'MGFGAKDKECANLLFKGHGLHKLKGKGKDCNLGSPKIFCRNIFFPSLKPTLKIQTQIASSFNLSITSTPVSITKSSLKPTLKIPTQITSISNLSISSTPVSVTKSSLKSTLSANPLQDPLYLTPRRHSDPSNAAALRRVSVVLFRTDLRVQDNESLTLANNESVSVLPVYCFDPNGFDKFDKSCNKVSFLIESVSDLRKNLQAKGSDLVVRGVGKDEVRGEKKVESGLKDEGVEVKWCWGGTLYHVDDLPFEVENMPSEYGGFREKVKGVKVRGAVECVDQLKGLPSCGNVEVGEIPSLADLGLGHTANMAQVKPVINGPLIGGETEALNRLKKFAAECQAHPPKQTNDGNNDSVYGASFSCKISPWLALGCISPRAVFDELKNSASRSFSAGSIGKDGGDSGMNWLMYELLWRDFFRFITWKYRSSKQHNSTPVPA']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_seqs = load_dataset('lhallee/triplets', split='valid')['positives']\n",
    "seqs = all_seqs[:4]\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embed_seqs(model, model_deep, tokenizer, seqs, device)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('local_emb_test.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6dafee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c4a96add2743c5a6e82b610ce096d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = ProteinVec.from_pretrained('lhallee/ProteinVec', config=ProteinVecConfig())\n",
    "model.to_eval()\n",
    "model = model.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('lhallee/ProteinVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7878152",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [(\" \".join(seqs[i])) for i in range(len(seqs))]\n",
    "sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences]\n",
    "ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=True)\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2286cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "emb = model.embed_batch(input_ids, attention_mask, aspect=6).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c2d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hf_emb_test.npy', emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d91634",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_emb = np.load('local_emb_test.npy')\n",
    "hf_emb = np.load('hf_emb_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e749fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.0215225 ,  6.265668  ,  4.1960397 , ...,  0.81374305,\n",
       "         5.500475  , -2.5042906 ],\n",
       "       [-8.844703  ,  6.1047983 ,  4.215445  , ...,  0.78629345,\n",
       "         5.840502  , -2.5396354 ],\n",
       "       [-8.900217  ,  6.1098895 ,  4.2216673 , ...,  1.0021304 ,\n",
       "         5.853938  , -2.494628  ],\n",
       "       [-8.937391  ,  6.2477336 ,  4.0384502 , ...,  0.830227  ,\n",
       "         5.735293  , -2.391416  ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0378936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.021523 ,  6.2656674,  4.196039 , ...,  0.8137429,  5.500475 ,\n",
       "        -2.5042913],\n",
       "       [-8.844703 ,  6.104799 ,  4.215445 , ...,  0.7862937,  5.8405027,\n",
       "        -2.5396352],\n",
       "       [-8.900217 ,  6.10989  ,  4.221667 , ...,  1.0021304,  5.853938 ,\n",
       "        -2.4946282],\n",
       "       [-8.937391 ,  6.247734 ,  4.038451 , ...,  0.8302269,  5.7352924,\n",
       "        -2.3914163]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4915bf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(local_emb, hf_emb, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba4329-557e-44e1-a0f3-19f0ade75326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in uniprot meta data\n",
    "meta_data_new = pd.read_csv('data/uniprotkb_AND_reviewed_true_2023_07_03.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03eabf-d1b2-4ca7-aadc-889d1eb8e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now filter for the proteins that were newly discovered\n",
    "new_proteins = meta_data_new[meta_data_new['Date of creation'] > '2022-05-25'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d2919-12be-4893-92d8-22c1db7ba41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of new proteins deposited after 2022-05-25')\n",
    "print(len(new_proteins))\n",
    "\n",
    "# filter those proteins that are greater than 2000 amino acid residues long\n",
    "new_proteins['length'] = new_proteins['Sequence'].str.len()\n",
    "new_proteins = new_proteins[new_proteins['length'] <= 2000]\n",
    "print('Filtered proteins longer than 2000 amino acids')\n",
    "print(len(new_proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc961b3-35ab-4b11-b284-6422f6b364b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a forward pass of the Protein-Vec model\n",
    "# Every aspect is turned on (therefore no masks)\n",
    "sampled_keys = np.array(['TM', 'PFAM', 'GENE3D', 'ENZYME', 'MFO', 'BPO', 'CCO'])\n",
    "all_cols = np.array(['TM', 'PFAM', 'GENE3D', 'ENZYME', 'MFO', 'BPO', 'CCO'])\n",
    "masks = [all_cols[k] in sampled_keys for k in range(len(all_cols))]\n",
    "masks = torch.logical_not(torch.tensor(masks, dtype=torch.bool))[None,:]\n",
    "\n",
    "#Pull out sequences for the new proteins\n",
    "flat_seqs = new_proteins['Sequence'].values\n",
    "\n",
    "#Loop through the sequences and embed them using protein-vec\n",
    "i = 0\n",
    "embed_all_sequences = []\n",
    "while i < len(flat_seqs): \n",
    "    protrans_sequence = featurize_prottrans(flat_seqs[i:i+1], model, tokenizer, device)\n",
    "    embedded_sequence = embed_vec(protrans_sequence, model_deep, masks, device)\n",
    "    embed_all_sequences.append(embedded_sequence)\n",
    "    i = i + 1\n",
    "    if i % 50 == 0:\n",
    "        print(i)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41299473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the embedding vectors into an array\n",
    "query_embeddings = np.concatenate(embed_all_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78713307-6ed2-435a-9389-18bb35902269",
   "metadata": {},
   "source": [
    "Now that we have embeddings for the newly discovered proteins, we can visualize them after performing TSNE, and we can transfer annotations to them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa34c5-d700-4320-af0d-d849636a4ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Perform TSNE on the embedding vectors\n",
    "all_X_embedded = TSNE(n_components=2, perplexity=10, learning_rate='auto', init='random').fit_transform(query_embeddings)\n",
    "all_X_embedded_df = pd.DataFrame(all_X_embedded)\n",
    "all_X_embedded_df.columns = [\"Dim1\", \"Dim2\"]\n",
    "all_X_embedded_df['Pfam'] = new_proteins['Pfam'].values[:len(all_X_embedded_df)]\n",
    "all_X_embedded_df['EC'] = new_proteins['EC number'].values[:len(all_X_embedded_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62818905-93bf-45b4-9d21-8842788b4a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For visualization purposes, filter for the top 20 PFam terms\n",
    "top_ranks = list(all_X_embedded_df['Pfam'].value_counts()[0:20].index)\n",
    "sns.lmplot(x=\"Dim1\", y=\"Dim2\", data=all_X_embedded_df[all_X_embedded_df['Pfam'].isin(top_ranks)], hue=\"Pfam\", fit_reg=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb6977-b852-4cf8-8c29-62ffe56358d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################## Load the lookup database of all embeddings (note that we will pull out only embeddings from proteins that were trained on)\n",
    "embeddings = np.load('protein_vec_embeddings/lookup_embeddings.npy')\n",
    "lookup_proteins_meta = pd.read_csv('protein_vec_embeddings/lookup_embeddings_meta_data.tsv', sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0443c1-bedf-47fc-8a9e-c9981b1ed7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Maximum date of lookup database protein\")\n",
    "np.max(lookup_proteins_meta['Date of creation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53d555-a20b-43c0-8140-15b88637e3a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can run search and the nearest neighbor pipeline for any of our available aspects\n",
    " - 'Gene Ontology (biological process)'\n",
    " - 'Gene Ontology (molecular function)' \n",
    " - 'Gene Ontology (cellular component)' \n",
    " - 'Gene3D' \n",
    " - 'Pfam' \n",
    " - 'EC number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd96ff-0f0a-4af5-a3d6-b599b1608443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch this for whichever aspect you want to perform search for\n",
    "############### User parameter\n",
    "column = 'Pfam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee8331-342c-4acf-b441-decd61c0b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for lookup proteins with annotations for the relavant aspect (don't want to transfer null annotations)\n",
    "col_lookup = lookup_proteins_meta[~lookup_proteins_meta[column].isnull()]\n",
    "col_lookup_embeddings = embeddings[col_lookup.index]\n",
    "col_meta_data = col_lookup[column].values\n",
    "\n",
    "# load database\n",
    "lookup_database = load_database(col_lookup_embeddings)\n",
    "\n",
    "# Query for the 1st nearest neighbor\n",
    "k = 1\n",
    "D, I = query(lookup_database, query_embeddings, k)\n",
    "\n",
    "#Get metadata for the 1st nearest neighbor\n",
    "near_ids = []\n",
    "for i in range(I.shape[0]):\n",
    "    meta = col_meta_data[I[i]]\n",
    "    near_ids.append(list(meta))       \n",
    "\n",
    "near_ids = np.array(near_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef3ae1-92a8-4618-90d4-7847c022c638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Annotations for the nearest neighbors (with aspect annotations) of newly discovered proteins\")\n",
    "print(near_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332d719-cad2-4c10-b5a2-6e45a1da82cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
