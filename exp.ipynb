{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d0c3bf3dfb4d6fb0f318da2e5b84db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/48.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736587b6bece42b0a1720e1346ff4c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9250d03418d45bdbf5ed36ffc7c3087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9d55c9dd0e49faace908662c39e5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/624M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e3e4e591614e8d828b57751e726755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at lhallee/ProteinVec were not used when initializing ProteinVec: ['t5.encoder.block.0.layer.0.SelfAttention.k.weight', 't5.encoder.block.0.layer.0.SelfAttention.o.weight', 't5.encoder.block.0.layer.0.SelfAttention.q.weight', 't5.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 't5.encoder.block.0.layer.0.SelfAttention.v.weight', 't5.encoder.block.0.layer.0.layer_norm.weight', 't5.encoder.block.0.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.0.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.0.layer.1.layer_norm.weight', 't5.encoder.block.1.layer.0.SelfAttention.k.weight', 't5.encoder.block.1.layer.0.SelfAttention.o.weight', 't5.encoder.block.1.layer.0.SelfAttention.q.weight', 't5.encoder.block.1.layer.0.SelfAttention.v.weight', 't5.encoder.block.1.layer.0.layer_norm.weight', 't5.encoder.block.1.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.1.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.1.layer.1.layer_norm.weight', 't5.encoder.block.10.layer.0.SelfAttention.k.weight', 't5.encoder.block.10.layer.0.SelfAttention.o.weight', 't5.encoder.block.10.layer.0.SelfAttention.q.weight', 't5.encoder.block.10.layer.0.SelfAttention.v.weight', 't5.encoder.block.10.layer.0.layer_norm.weight', 't5.encoder.block.10.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.10.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.10.layer.1.layer_norm.weight', 't5.encoder.block.11.layer.0.SelfAttention.k.weight', 't5.encoder.block.11.layer.0.SelfAttention.o.weight', 't5.encoder.block.11.layer.0.SelfAttention.q.weight', 't5.encoder.block.11.layer.0.SelfAttention.v.weight', 't5.encoder.block.11.layer.0.layer_norm.weight', 't5.encoder.block.11.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.11.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.11.layer.1.layer_norm.weight', 't5.encoder.block.12.layer.0.SelfAttention.k.weight', 't5.encoder.block.12.layer.0.SelfAttention.o.weight', 't5.encoder.block.12.layer.0.SelfAttention.q.weight', 't5.encoder.block.12.layer.0.SelfAttention.v.weight', 't5.encoder.block.12.layer.0.layer_norm.weight', 't5.encoder.block.12.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.12.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.12.layer.1.layer_norm.weight', 't5.encoder.block.13.layer.0.SelfAttention.k.weight', 't5.encoder.block.13.layer.0.SelfAttention.o.weight', 't5.encoder.block.13.layer.0.SelfAttention.q.weight', 't5.encoder.block.13.layer.0.SelfAttention.v.weight', 't5.encoder.block.13.layer.0.layer_norm.weight', 't5.encoder.block.13.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.13.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.13.layer.1.layer_norm.weight', 't5.encoder.block.14.layer.0.SelfAttention.k.weight', 't5.encoder.block.14.layer.0.SelfAttention.o.weight', 't5.encoder.block.14.layer.0.SelfAttention.q.weight', 't5.encoder.block.14.layer.0.SelfAttention.v.weight', 't5.encoder.block.14.layer.0.layer_norm.weight', 't5.encoder.block.14.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.14.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.14.layer.1.layer_norm.weight', 't5.encoder.block.15.layer.0.SelfAttention.k.weight', 't5.encoder.block.15.layer.0.SelfAttention.o.weight', 't5.encoder.block.15.layer.0.SelfAttention.q.weight', 't5.encoder.block.15.layer.0.SelfAttention.v.weight', 't5.encoder.block.15.layer.0.layer_norm.weight', 't5.encoder.block.15.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.15.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.15.layer.1.layer_norm.weight', 't5.encoder.block.16.layer.0.SelfAttention.k.weight', 't5.encoder.block.16.layer.0.SelfAttention.o.weight', 't5.encoder.block.16.layer.0.SelfAttention.q.weight', 't5.encoder.block.16.layer.0.SelfAttention.v.weight', 't5.encoder.block.16.layer.0.layer_norm.weight', 't5.encoder.block.16.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.16.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.16.layer.1.layer_norm.weight', 't5.encoder.block.17.layer.0.SelfAttention.k.weight', 't5.encoder.block.17.layer.0.SelfAttention.o.weight', 't5.encoder.block.17.layer.0.SelfAttention.q.weight', 't5.encoder.block.17.layer.0.SelfAttention.v.weight', 't5.encoder.block.17.layer.0.layer_norm.weight', 't5.encoder.block.17.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.17.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.17.layer.1.layer_norm.weight', 't5.encoder.block.18.layer.0.SelfAttention.k.weight', 't5.encoder.block.18.layer.0.SelfAttention.o.weight', 't5.encoder.block.18.layer.0.SelfAttention.q.weight', 't5.encoder.block.18.layer.0.SelfAttention.v.weight', 't5.encoder.block.18.layer.0.layer_norm.weight', 't5.encoder.block.18.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.18.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.18.layer.1.layer_norm.weight', 't5.encoder.block.19.layer.0.SelfAttention.k.weight', 't5.encoder.block.19.layer.0.SelfAttention.o.weight', 't5.encoder.block.19.layer.0.SelfAttention.q.weight', 't5.encoder.block.19.layer.0.SelfAttention.v.weight', 't5.encoder.block.19.layer.0.layer_norm.weight', 't5.encoder.block.19.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.19.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.19.layer.1.layer_norm.weight', 't5.encoder.block.2.layer.0.SelfAttention.k.weight', 't5.encoder.block.2.layer.0.SelfAttention.o.weight', 't5.encoder.block.2.layer.0.SelfAttention.q.weight', 't5.encoder.block.2.layer.0.SelfAttention.v.weight', 't5.encoder.block.2.layer.0.layer_norm.weight', 't5.encoder.block.2.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.2.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.2.layer.1.layer_norm.weight', 't5.encoder.block.20.layer.0.SelfAttention.k.weight', 't5.encoder.block.20.layer.0.SelfAttention.o.weight', 't5.encoder.block.20.layer.0.SelfAttention.q.weight', 't5.encoder.block.20.layer.0.SelfAttention.v.weight', 't5.encoder.block.20.layer.0.layer_norm.weight', 't5.encoder.block.20.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.20.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.20.layer.1.layer_norm.weight', 't5.encoder.block.21.layer.0.SelfAttention.k.weight', 't5.encoder.block.21.layer.0.SelfAttention.o.weight', 't5.encoder.block.21.layer.0.SelfAttention.q.weight', 't5.encoder.block.21.layer.0.SelfAttention.v.weight', 't5.encoder.block.21.layer.0.layer_norm.weight', 't5.encoder.block.21.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.21.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.21.layer.1.layer_norm.weight', 't5.encoder.block.22.layer.0.SelfAttention.k.weight', 't5.encoder.block.22.layer.0.SelfAttention.o.weight', 't5.encoder.block.22.layer.0.SelfAttention.q.weight', 't5.encoder.block.22.layer.0.SelfAttention.v.weight', 't5.encoder.block.22.layer.0.layer_norm.weight', 't5.encoder.block.22.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.22.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.22.layer.1.layer_norm.weight', 't5.encoder.block.23.layer.0.SelfAttention.k.weight', 't5.encoder.block.23.layer.0.SelfAttention.o.weight', 't5.encoder.block.23.layer.0.SelfAttention.q.weight', 't5.encoder.block.23.layer.0.SelfAttention.v.weight', 't5.encoder.block.23.layer.0.layer_norm.weight', 't5.encoder.block.23.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.23.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.23.layer.1.layer_norm.weight', 't5.encoder.block.3.layer.0.SelfAttention.k.weight', 't5.encoder.block.3.layer.0.SelfAttention.o.weight', 't5.encoder.block.3.layer.0.SelfAttention.q.weight', 't5.encoder.block.3.layer.0.SelfAttention.v.weight', 't5.encoder.block.3.layer.0.layer_norm.weight', 't5.encoder.block.3.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.3.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.3.layer.1.layer_norm.weight', 't5.encoder.block.4.layer.0.SelfAttention.k.weight', 't5.encoder.block.4.layer.0.SelfAttention.o.weight', 't5.encoder.block.4.layer.0.SelfAttention.q.weight', 't5.encoder.block.4.layer.0.SelfAttention.v.weight', 't5.encoder.block.4.layer.0.layer_norm.weight', 't5.encoder.block.4.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.4.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.4.layer.1.layer_norm.weight', 't5.encoder.block.5.layer.0.SelfAttention.k.weight', 't5.encoder.block.5.layer.0.SelfAttention.o.weight', 't5.encoder.block.5.layer.0.SelfAttention.q.weight', 't5.encoder.block.5.layer.0.SelfAttention.v.weight', 't5.encoder.block.5.layer.0.layer_norm.weight', 't5.encoder.block.5.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.5.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.5.layer.1.layer_norm.weight', 't5.encoder.block.6.layer.0.SelfAttention.k.weight', 't5.encoder.block.6.layer.0.SelfAttention.o.weight', 't5.encoder.block.6.layer.0.SelfAttention.q.weight', 't5.encoder.block.6.layer.0.SelfAttention.v.weight', 't5.encoder.block.6.layer.0.layer_norm.weight', 't5.encoder.block.6.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.6.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.6.layer.1.layer_norm.weight', 't5.encoder.block.7.layer.0.SelfAttention.k.weight', 't5.encoder.block.7.layer.0.SelfAttention.o.weight', 't5.encoder.block.7.layer.0.SelfAttention.q.weight', 't5.encoder.block.7.layer.0.SelfAttention.v.weight', 't5.encoder.block.7.layer.0.layer_norm.weight', 't5.encoder.block.7.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.7.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.7.layer.1.layer_norm.weight', 't5.encoder.block.8.layer.0.SelfAttention.k.weight', 't5.encoder.block.8.layer.0.SelfAttention.o.weight', 't5.encoder.block.8.layer.0.SelfAttention.q.weight', 't5.encoder.block.8.layer.0.SelfAttention.v.weight', 't5.encoder.block.8.layer.0.layer_norm.weight', 't5.encoder.block.8.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.8.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.8.layer.1.layer_norm.weight', 't5.encoder.block.9.layer.0.SelfAttention.k.weight', 't5.encoder.block.9.layer.0.SelfAttention.o.weight', 't5.encoder.block.9.layer.0.SelfAttention.q.weight', 't5.encoder.block.9.layer.0.SelfAttention.v.weight', 't5.encoder.block.9.layer.0.layer_norm.weight', 't5.encoder.block.9.layer.1.DenseReluDense.wi.weight', 't5.encoder.block.9.layer.1.DenseReluDense.wo.weight', 't5.encoder.block.9.layer.1.layer_norm.weight', 't5.encoder.final_layer_norm.weight', 't5.shared.weight']\n",
      "- This IS expected if you are initializing ProteinVec from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ProteinVec from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProteinVec(\n",
      "  (moe): trans_basic_block(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (mlp_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (trip_margin_loss): TripletMarginLoss()\n",
      "    (cos): CosineSimilarity()\n",
      "    (l1_loss): L1Loss()\n",
      "    (pdist): PairwiseDistance()\n",
      "    (model_aspect_tmvec): trans_basic_block_tmvec(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "    )\n",
      "    (model_aspect_pfam): trans_basic_block_single(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (trip_margin_loss): TripletMarginLoss()\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "      (pdist): PairwiseDistance()\n",
      "    )\n",
      "    (model_aspect_gene3D): trans_basic_block_single(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (trip_margin_loss): TripletMarginLoss()\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "      (pdist): PairwiseDistance()\n",
      "    )\n",
      "    (model_aspect_ec): trans_basic_block_single(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (trip_margin_loss): TripletMarginLoss()\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "      (pdist): PairwiseDistance()\n",
      "    )\n",
      "    (model_aspect_mfo): trans_basic_block_single(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (trip_margin_loss): TripletMarginLoss()\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "      (pdist): PairwiseDistance()\n",
      "    )\n",
      "    (model_aspect_bpo): trans_basic_block_single(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (trip_margin_loss): TripletMarginLoss()\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "      (pdist): PairwiseDistance()\n",
      "    )\n",
      "    (model_aspect_cco): trans_basic_block_single(\n",
      "      (encoder): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (mlp_1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (mlp_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (trip_margin_loss): TripletMarginLoss()\n",
      "      (cos): CosineSimilarity()\n",
      "      (l1_loss): L1Loss()\n",
      "      (pdist): PairwiseDistance()\n",
      "    )\n",
      "  )\n",
      "  (contrastive_loss): TripletMarginLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4406eb3ed5401fa2f16bf4c6e735a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating sim metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'trans_basic_block_Config' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\Research\\Mixture-of-Experts-Sentence-Similarity\\main.py:81\u001b[0m\n\u001b[0;32m     76\u001b[0m             train_sim_model(yargs, model, tokenizer,\n\u001b[0;32m     77\u001b[0m                             compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics_sentence_similarity, token\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtoken)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Research\\Mixture-of-Experts-Sentence-Similarity\\main.py:44\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproteinvec\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[43mevaluate_protein_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43myargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit()\n",
      "File \u001b[1;32m~\\Desktop\\Research\\Mixture-of-Experts-Sentence-Similarity\\evaluate.py:168\u001b[0m, in \u001b[0;36mevaluate_protein_vec\u001b[1;34m(yargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\Desktop\\Research\\Mixture-of-Experts-Sentence-Similarity\\evaluate.py:61\u001b[0m, in \u001b[0;36mevaluate_triplet_model_similarity\u001b[1;34m(yargs, model, tokenizer)\u001b[0m\n\u001b[0;32m     59\u001b[0m triplet_datasets \u001b[38;5;241m=\u001b[39m get_datasets_test_triplet(args, tokenizer) \u001b[38;5;66;03m# (aspect, valid_dataset, test_dataset) * aspects * num_dataset\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (aspect, valid_dataset, test_dataset) \u001b[38;5;129;01min\u001b[39;00m tqdm(triplet_datasets, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating sim metrics\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 61\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     log_metrics(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_path\u001b[39m\u001b[38;5;124m'\u001b[39m], metrics, details\u001b[38;5;241m=\u001b[39margs, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid aspect \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maspect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset)\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3418\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3415\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 3418\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3419\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3420\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3635\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   3633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[0;32m   3634\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3635\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3636\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   3638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:817\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\operations.py:805\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Research\\Mixture-of-Experts-Sentence-Similarity\\models\\protein_vec\\src_run\\EMBED_w_pvec.py:61\u001b[0m, in \u001b[0;36mProteinVec.forward\u001b[1;34m(self, pos, anc, neg, att_p, att_a, att_n, r_labels)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos, anc, neg,\n\u001b[0;32m     60\u001b[0m             att_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, att_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, att_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, r_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 61\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matt_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch(input_ids\u001b[38;5;241m=\u001b[39manc, attention_mask\u001b[38;5;241m=\u001b[39matt_a, aspect\u001b[38;5;241m=\u001b[39mr_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     63\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch(input_ids\u001b[38;5;241m=\u001b[39mneg, attention_mask\u001b[38;5;241m=\u001b[39matt_n, aspect\u001b[38;5;241m=\u001b[39mr_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\Desktop\\Research\\Mixture-of-Experts-Sentence-Similarity\\models\\protein_vec\\src_run\\EMBED_w_pvec.py:47\u001b[0m, in \u001b[0;36mProteinVec.embed_batch\u001b[1;34m(self, input_ids, attention_mask, aspect)\u001b[0m\n\u001b[0;32m     45\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mask(aspect)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 47\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     48\u001b[0m vecs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embedding)):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'trans_basic_block_Config' object is not callable"
     ]
    }
   ],
   "source": [
    "%run main --yaml_path yamls/protein_vec.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Load Model-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForTripletSimilarity(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "              (dropout): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=320, out_features=320, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (contrastive_loss): TripletMarginLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e4ee8d255340219eb665b68ea49b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/623 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415f81a3a8ac406d89f76016cb36f132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f1341d10b747f8a4ff83733b5b6f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/82.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889e01b802f8443e95b36595913a46e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6c2bd61cd84d9d850f20ed7132ad66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181d8e57a2df4499a532f080894a491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/397M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818983c3edc7420c9453c35d28bcea09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ef28a3bad247759416a6482fb9c146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/49667 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c279bdc3ad493496dd605b7462e567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/807412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137d8961d3684a5a822e58790563951a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating sim metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09009ec0fff946e9a406255c050439ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9413bc190c52495681c392c7e44adb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ec61220e6048c49fb66abd3c7cf5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f3f62dac4d4362b40f4e7b05b548b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bdd6a85b434d319157dc86842ac777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992fb0b8fa9f4fb7a3ac5f4013d91e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e34da9553b74070a97befb44cd0eba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6dc1b81fd554af9bd0bde5fb09bec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf9599b0602426893033094981ed976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b759c76fcc194a328a411c94c53b4eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e861ea6cc34d7a80f905ade37b1c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70951f82af80420b80067aa289d98a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --eval --yaml_path yamls/benchmarking/esm_sim_benchmark_8.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Load Model-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForTripletSimilarity(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (dropout): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=480, out_features=480, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=240, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (contrastive_loss): TripletMarginLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21def084710457d968663e5a7df8a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating sim metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f2f4f067b54107b93b1faea0a12acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7766414db9641a392cb889d675327b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1032 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bd0552f7034eddb7feabf348ad28af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746270ae8a554636823f52651485617c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d60a5cb829d4c11927f78b9e291e8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9fcb9a010245aebae91c4be6344c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79d93572e304d299b941b47706858bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef10a75460924f8dbaa2b3eef7dde5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8fa96a07b34e7392c29339676d4748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a578c14a91419eb928542a67e8f2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23258517fba4ff29640dd1f64ed5458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf1c3e32e6149d39e0c860c94759894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --eval --yaml_path yamls/benchmarking/esm_sim_benchmark_35.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Load Model-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForTripletSimilarity(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 640, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-29): 30 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=640, out_features=640, bias=True)\n",
      "              (key): Linear(in_features=640, out_features=640, bias=True)\n",
      "              (value): Linear(in_features=640, out_features=640, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=640, out_features=640, bias=True)\n",
      "              (dropout): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=640, out_features=2560, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=2560, out_features=640, bias=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=640, out_features=640, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=600, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (contrastive_loss): TripletMarginLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7a569e247b41cdac25d1f307671c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating sim metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b5aededc9943e2b63ffe1a89bbf9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b76f3ed7a52486ab2d45ae729e0de67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4273f9f30c4593b934dfd1799c62f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f6712900504133b079be417c57052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2083 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d42c00cb3144127accea421cfe6b1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff86a83900442a1b7742d0ffba4ff83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6727511e7a1f4d6c974444775b112fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b596cbfbd4e40c7bcf37224bc51493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea68333adec4be9ae7df563fbba8e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce40912fdc49491990205adcf9b895a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71735982df574367840ad06af2cbed6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343aa5377519454fba3fa812f8e415f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --eval --yaml_path yamls/benchmarking/esm_sim_benchmark_150.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Load Model-----\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForTripletSimilarity(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-32): 33 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.05, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (contrastive_loss): TripletMarginLoss()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4345f5395a464ca08b638dc5eff61fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating sim metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c84715fd195417996009bca0819468c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e190bacc453c4840b0c3df71a4c01222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c87914276642548b84587f72be424f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45955c335154a0989468fa94f0547d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e44239d6d84eb997814b6539c4971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb13c3542a4a97ae006d4e39cc4e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e222afac200a411c90e7e0cdea868c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58b83afbdf842df8e6d13aafabd8331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f792936892d4d91a47069c83ca7632a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b50f02be1c480e99a2a73eb667bb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d02a76c2d640559763ac3c18d3e20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fb5d2fddf54f45982e757ed25c60bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run main.py --eval --yaml_path yamls/benchmarking/esm_sim_benchmark_650.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from models.model_zoo import *\n",
    "from utils import get_yaml\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yargs = get_yaml('yamls/SE/cvd.yaml')\n",
    "config = BertConfig.from_pretrained(yargs['general_args']['model_path'])\n",
    "for key, value in yargs['general_args'].items(): # copy yaml config into args\n",
    "    setattr(config, key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [path.replace('\\\\', '/') for path in glob('./weights/*.pt')]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for weight in weights:\n",
    "    model = torch.load(weight)\n",
    "    try:\n",
    "        model.config = model.bert.config\n",
    "    except:\n",
    "        print(weight)\n",
    "        continue\n",
    "    path = 'lhallee/' + weight.split('/')[-1].split('.pt')[0]\n",
    "    paths.append(path)\n",
    "    #model.push_to_hub(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('results/NLP_MOE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    tokenizer.push_to_hub(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('lhallee/sci_moe_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub('lhallee/all_nlp_singlemoe_MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
