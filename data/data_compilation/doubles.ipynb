{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict, Counter\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset, DatasetDict\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2311b6ddbb5343aba8e3ebedd794aef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, delimiter='\\t').rename(columns={\n",
    "        'EC number':'EC',\n",
    "        'Gene Ontology (molecular function)':'MF',\n",
    "        'Gene Ontology (biological process)':'BP',\n",
    "        'Gene Ontology (cellular component)':'CC',\n",
    "        'Sequence':'seqs'\n",
    "    }).astype('string')\n",
    "    print(len(df))\n",
    "    df['combined'] = df.progress_apply(lambda x: ' '.join(str(x[col]) for col in df.columns if col != 'seqs'), axis=1)\n",
    "    df = df.sort_values(by='combined', key=lambda x: x.str.len(), ascending=False)\n",
    "    df = df.drop_duplicates(subset='seqs', keep='first')\n",
    "    df = df.drop('combined', axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_dictionary(input, start=1, name='ec'):\n",
    "    id2label, label2id = {}, {}\n",
    "    for index, entry in enumerate(input, start=start):\n",
    "        entry = entry + '_' + name\n",
    "        id2label[index] = entry\n",
    "        label2id[entry] = index\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def process_descriptors(input_list,\n",
    "                        start=1,\n",
    "                        name='ec',\n",
    "                        id2label=None,\n",
    "                        label2id=None,\n",
    "                        filter_func=lambda d: d.strip()):\n",
    "    col_list, new_col = [], []\n",
    "\n",
    "    if id2label == None or label2id == None:\n",
    "        for item in tqdm(input_list, desc=f'{name} make dicts'):\n",
    "            descriptors = str(item).split(';')\n",
    "            filtered_descriptors = [filter_func(d) for d in descriptors]\n",
    "            filtered_descriptors = [d for d in filtered_descriptors if d and d.lower() != 'none' and d.lower() != 'nan']\n",
    "            col_list.extend(filtered_descriptors)\n",
    "        col_list = sorted(list(set(col_list)))\n",
    "        if '' in col_list:\n",
    "            col_list.remove('')\n",
    "        len_col_list = len(col_list)\n",
    "        id2label, label2id = create_dictionary(col_list, start=start, name=name)\n",
    "    else:\n",
    "        len_col_list = len(id2label.keys())\n",
    "\n",
    "    for item in tqdm(input_list, desc=f'{name} make new column'):\n",
    "        descriptors = str(item).split(';')\n",
    "        filtered_descriptors = [filter_func(d) for d in descriptors]\n",
    "        filtered_descriptors = [d for d in filtered_descriptors if d and d.lower() != 'none' and d.lower() != 'nan']\n",
    "        new_entry = [label2id[d + '_' + name] for d in filtered_descriptors if d] or [0]\n",
    "        new_col.append(new_entry)\n",
    "\n",
    "    return new_col, id2label, label2id, len_col_list + start\n",
    "\n",
    "\n",
    "def ec_processing(input_list, start=1, name='ec', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id,\n",
    "                               filter_func=lambda d: d.strip() if '-' not in d and 'n' not in d else '')\n",
    "\n",
    "\n",
    "def go_processing(input_list, start=1, name='go', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id,\n",
    "                               filter_func=lambda d: d[d.find('[GO:')+1:d.find(']')].strip())\n",
    "\n",
    "\n",
    "def cofactor_processing(input_list, start=1, name='co', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id,\n",
    "                               filter_func=lambda d: d[d.find('Name=')+5:].strip() if 'Name' in d else '')\n",
    "\n",
    "\n",
    "def domain_processing(input_list, start=1, name='ip', id2label=None, label2id=None):\n",
    "    return process_descriptors(input_list, start=start, name=name, id2label=id2label, label2id=label2id)\n",
    "\n",
    "\n",
    "def replace_df(df, all_cols):\n",
    "    new_ec_col, new_mf_col, new_bp_col, new_cc_col = all_cols\n",
    "    combined_list = [sorted([item for item in list(chain.from_iterable(element)) if item != 0])\n",
    "                     for element in zip(*all_cols) if element != [0]]\n",
    "\n",
    "    df['EC'] = new_ec_col\n",
    "    df['MF'] = new_mf_col\n",
    "    df['BP'] = new_bp_col\n",
    "    df['CC'] = new_cc_col\n",
    "    df['combined'] = combined_list\n",
    "    df['string_combined'] = df['combined'].astype('string')\n",
    "\n",
    "    unique_dict = {}\n",
    "\n",
    "    # Iterate over the DataFrame and update the dictionary\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        combined_value = row['string_combined']\n",
    "        if combined_value not in unique_dict:\n",
    "            unique_dict[combined_value] = row\n",
    "        if combined_value == '[0]':\n",
    "            print(combined_value)\n",
    "\n",
    "    # Create the final DataFrame from the dictionary values\n",
    "    df_final = pd.DataFrame(unique_dict.values()).drop(columns=['string_combined'])\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce75c91ddda84d72a18755146c6d4093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4814386230cb4a4aa90fddc7bde47b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ec make dicts:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbc9376fd3b413ab4e8529657b00398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ec make new column:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c65a5e20e24229ad6a13bee065eed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mf make dicts:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56cf299934648edb6d93f49ee34b4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mf make new column:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3206e660d25438197af3f425ccff7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bp make dicts:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42604e363f944398ae2e17f394574b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bp make new column:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cac358ce367440f89b827c527aa6974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cc make dicts:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b381f20cd343cca3a76f7d00049366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cc make new column:   0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24156\n",
      "24156\n",
      "24156 24156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f894b81547d4840b90069af33629599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55163\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "df = load_df('doubles.tsv')\n",
    "# EC\n",
    "ecs = df['EC'].tolist()\n",
    "new_ec_col, id2ec, ec2id, ec_len = ec_processing(ecs, name='ec', start=1)\n",
    "# MF\n",
    "mfs = df['MF'].tolist()\n",
    "new_mf_col, id2mf, mf2id, mf_len = go_processing(mfs, name='mf', start=ec_len+1)\n",
    "# BP\n",
    "bps = df['BP'].tolist()\n",
    "new_bp_col, id2bp, bp2id, bp_len = go_processing(bps, name='bp', start=mf_len+1)\n",
    "# CC\n",
    "ccs = df['CC'].tolist()\n",
    "new_cc_col, id2cc, cc2id, cc_len = go_processing(ccs, name='cc', start=bp_len+1)\n",
    "\n",
    "# make full dicts and check for no duplicates\n",
    "all_id = [id2ec, id2mf, id2bp, id2cc]\n",
    "all_label = [ec2id, mf2id, bp2id, cc2id]\n",
    "\n",
    "id2label, label2id = {}, {}\n",
    "\n",
    "key_counts = 0\n",
    "for d in all_id:\n",
    "    key_counts += len(d.keys())\n",
    "    id2label.update(d)\n",
    "print(key_counts)\n",
    "\n",
    "key_counts = 0\n",
    "for d in all_label:\n",
    "    key_counts += len(d.keys())\n",
    "    label2id.update(d)\n",
    "print(key_counts)\n",
    "\n",
    "for k, v in id2label.items():\n",
    "    if k != label2id[v]:\n",
    "        print(v)\n",
    "\n",
    "for k, v in label2id.items():\n",
    "    if k != id2label[v]:\n",
    "        print(v)\n",
    "\n",
    "print(len(id2label.keys()), len(label2id.keys()))\n",
    "\n",
    "all_cols = [new_ec_col, new_mf_col, new_bp_col, new_cc_col]\n",
    "\n",
    "df_final = replace_df(df, all_cols)\n",
    "\n",
    "print(len(df_final))\n",
    "\n",
    "df_final.to_csv('processed_doubles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "df = load_df('trembl_all_aspects_raw.tsv')\n",
    "# EC\n",
    "ecs = df['EC'].tolist()\n",
    "new_ec_col, id2ec, ec2id, ec_len = ec_processing(ecs, name='ec', start=1)\n",
    "# MF\n",
    "mfs = df['MF'].tolist()\n",
    "new_mf_col, id2mf, mf2id, mf_len = go_processing(mfs, name='mf', start=ec_len+1)\n",
    "# BP\n",
    "bps = df['BP'].tolist()\n",
    "new_bp_col, id2bp, bp2id, bp_len = go_processing(bps, name='bp', start=mf_len+1)\n",
    "# CC\n",
    "ccs = df['CC'].tolist()\n",
    "new_cc_col, id2cc, cc2id, cc_len = go_processing(ccs, name='cc', start=bp_len+1)\n",
    "# IP\n",
    "ips = df['IP'].tolist()\n",
    "new_ip_col, id2ip, ip2id, ip_len = domain_processing(ips, name='ip', start=cc_len+1)\n",
    "# 3D\n",
    "threeds = df['3D'].tolist()\n",
    "new_threed_col, id2threed, threed2id, threed_len = domain_processing(threeds, name='3d', start=ip_len+1)\n",
    "# cofactor\n",
    "cos = df['Cofactor'].tolist()\n",
    "new_co_col, id2co, co2id, co_len = cofactor_processing(cos, name='co', start=threed_len+1)\n",
    "\n",
    "\n",
    "all_cols = [new_ec_col, new_mf_col, new_bp_col, new_cc_col, new_ip_col, new_threed_col, new_co_col]\n",
    "df = replace_df(df, all_cols)\n",
    "\n",
    "# remove dups\n",
    "for col in df.columns:\n",
    "    mask = df[col].apply(lambda x: x != [0]\n",
    "                        and (not isinstance(x, list) or len(x) > 0)\n",
    "                        and (not isinstance(x, str) or x.strip() != '[]'))\n",
    "    df = df.loc[mask]\n",
    "\n",
    "df_filtered = df[~df['seqs'].isin(df_final['seqs'])]\n",
    "\n",
    "print(len(df_filtered))\n",
    "df_filtered.to_csv('processed_trembl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Entry Entry Name                                               seqs   EC  \\\n",
      "0  Q62226  SHH_MOUSE  MLLLLARCFLVILASSLLVCPGLACGPGRGFGKRRHPKKLTPLAYK...  [0]   \n",
      "\n",
      "                                                  BP  \\\n",
      "0  [17253, 17689, 17583, 12294, 10703, 17582, 127...   \n",
      "\n",
      "                                                  CC  \\\n",
      "0  [22992, 22830, 22993, 22620, 22623, 22624, 230...   \n",
      "\n",
      "                                            MF  \\\n",
      "0  [5471, 10288, 5485, 7660, 5396, 5567, 5585]   \n",
      "\n",
      "                                            combined  \n",
      "0  {10753, 17922, 21507, 22532, 12294, 22536, 230...  \n"
     ]
    }
   ],
   "source": [
    "# Loading previously processed\n",
    "df = pd.read_csv('processed_doubles.csv', converters={\n",
    "    'EC': literal_eval,\n",
    "    'MF': literal_eval,\n",
    "    'BP': literal_eval,\n",
    "    'CC': literal_eval,\n",
    "    'combined': literal_eval\n",
    "})\n",
    "\n",
    "df['combined'] = df['combined'].apply(set)\n",
    "\n",
    "ec_aspect = df['EC'].apply(set).tolist()\n",
    "mf_aspect = df['MF'].apply(set).tolist()\n",
    "bp_aspect = df['BP'].apply(set).tolist()\n",
    "cc_aspect = df['CC'].apply(set).tolist()\n",
    "\n",
    "single_aspect_dict = {\n",
    "    'EC': ec_aspect,\n",
    "    'MF': mf_aspect,\n",
    "    'BP': bp_aspect,\n",
    "    'CC': cc_aspect,\n",
    "}\n",
    "\n",
    "print(df.head(1))\n",
    "\n",
    "sets = df['combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pairs(seqs, indices):\n",
    "    A, B = [], []\n",
    "    for i, j in indices:\n",
    "        A.append(seqs[i])\n",
    "        B.append(seqs[j])\n",
    "    return A, B\n",
    "\n",
    "\n",
    "def flatten_and_count(tuples_list):\n",
    "    flattened_list = [item for tuple_item in tuples_list for item in tuple_item]\n",
    "    element_counts = Counter(flattened_list)\n",
    "    most_repeated_element, count = element_counts.most_common(1)[0]\n",
    "    return most_repeated_element, count\n",
    "\n",
    "\n",
    "def make_pairs_nonredundant(pairs, max_count=10):\n",
    "    index_count = defaultdict(int)\n",
    "    seen_pairs = set()\n",
    "    nonredundant_pairs = []\n",
    "    sorted_pairs = sorted(pairs, key=lambda x: index_count[x[0]] + index_count[x[1]])\n",
    "    for pair in tqdm(sorted_pairs, desc='Making non redundant'):\n",
    "        i, j = pair\n",
    "        if (i, j) not in seen_pairs and (j, i) not in seen_pairs:\n",
    "            if index_count[i] < max_count and index_count[j] < max_count:\n",
    "                nonredundant_pairs.append(pair)\n",
    "                seen_pairs.add((i, j))\n",
    "                index_count[i] += 1\n",
    "                index_count[j] += 1\n",
    "    return nonredundant_pairs\n",
    "\n",
    "\n",
    "def calculate_similar_sets(sets):\n",
    "    pairs_10, pairs_30, pairs_50, pairs_70 = [], [], [], []\n",
    "    count_10, count_30, count_50, count_70 = 0, 0, 0, 0\n",
    "    len_sets = len(sets)\n",
    "    set_range = set(list(range(len_sets)))\n",
    "    for i in tqdm(set_range, desc='Measuring set similarity'):\n",
    "        set_i = sets[i]\n",
    "        new_range = set_range - {i}\n",
    "        for j in new_range:\n",
    "            set_j = sets[j]\n",
    "            intersection = set_i & set_j\n",
    "            union = set_i.union(set_j)\n",
    "            similarity = len(intersection) / len(union)\n",
    "            if similarity >= 0.1:\n",
    "                count_10 += 1\n",
    "                pairs_10.append((i, j))\n",
    "            if similarity >= 0.3:\n",
    "                count_30 += 1\n",
    "                pairs_30.append((i, j))\n",
    "            if similarity >= 0.5:\n",
    "                count_50 += 1\n",
    "                pairs_50.append((i, j))\n",
    "            if similarity >= 0.7:\n",
    "                count_70 += 1\n",
    "                pairs_70.append((i, j))\n",
    "    pairs = (pairs_10, pairs_30, pairs_50, pairs_70)\n",
    "    counts = (count_10, count_30, count_50, count_70)\n",
    "    return pairs, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1817436e248e4070b8d932fc490663a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percents\n",
      "4.61010023702379%\n",
      "0.1551575874627608%\n",
      "0.026323017822867913%\n",
      "0.004578704849730637%\n",
      "Redundant pair length\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "pairs, counts = calculate_similar_sets(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percents\n",
      "4.61010023702379%\n",
      "0.1551575874627608%\n",
      "0.026323017822867913%\n",
      "0.004578704849730637%\n",
      "Redundant pair length\n",
      "140.283348\n",
      "4.721378\n",
      "0.800998\n",
      "0.139328\n"
     ]
    }
   ],
   "source": [
    "print('Percents')\n",
    "for count in counts:\n",
    "    print(f'{count * 100 / (len(sets) ** 2)}%')\n",
    "print('Redundant pair length')\n",
    "for pair in pairs:\n",
    "    print(len(pair) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb12b87538a0487c9eebb95a2a0ea17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making non redundant:   0%|          | 0/140283348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ffa7ce426b4e62bbec164b117c7511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making non redundant:   0%|          | 0/4721378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b9aa9da9b2462f818cfcec1bf941fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making non redundant:   0%|          | 0/800998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743b26c72d7c4af396881062b15762ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making non redundant:   0%|          | 0/139328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(275452, 234053, 158003, 61766)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_10 = make_pairs_nonredundant(pairs[0])\n",
    "non_30 = make_pairs_nonredundant(pairs[1])\n",
    "non_50 = make_pairs_nonredundant(pairs[2])\n",
    "non_70 = make_pairs_nonredundant(pairs[3])\n",
    "len(non_10), len(non_30), len(non_50), len(non_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLLLLARCFLVILASSLLVCPGLACGPGRGFGKRRHPKKLTPLAYKQFIPNVAEKTLGASGRYEGKITRNSERFKELTPNYNPDIIFKDEENTGADRLMTQRCKDKLNALAISVMNQWPGVKLRVTEGWDEDGHHSEESLHYEGRAVDITTSDRDRSKYGMLARLAVEAGFDWVYYESKAHIHCSVKAENSVAAKSGGCFPGSATVHLEQGGTKLVKDLRPGDRVLAADDQGRLLYSDFLTFLDRDEGAKKVFYVIETLEPRERLLLTAAHLLFVAPHNDSGPTPGPSALFASRVRPGQRVYVVAERGGDRRLLPAAVHSVTLREEEAGAYAPLTAHGTILINRVLASCYAVIEEHSWAHRAFAPFRLAHALLAALAPARTDGGGGGSIPAAQSATEARGAEPTAGIHWYSQLLYHIGTWLLDSETMHPLGMAVKSS'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = df['seqs'].tolist()\n",
    "seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    10: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 275452\n",
       "    })\n",
       "    30: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 234053\n",
       "    })\n",
       "    50: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 158003\n",
       "    })\n",
       "    70: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 61766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a10, b10 = retrieve_pairs(seqs, non_10)\n",
    "a30, b30 = retrieve_pairs(seqs, non_30)\n",
    "a50, b50 = retrieve_pairs(seqs, non_50)\n",
    "a70, b70 = retrieve_pairs(seqs, non_70)\n",
    "\n",
    "data = DatasetDict({\n",
    "    '10': Dataset.from_dict({\n",
    "        'a': a10,\n",
    "        'b': b10\n",
    "    }),\n",
    "    '30': Dataset.from_dict({\n",
    "        'a': a30,\n",
    "        'b': b30\n",
    "    }),\n",
    "    '50': Dataset.from_dict({\n",
    "        'a': a50,\n",
    "        'b': b50\n",
    "    }),\n",
    "    '70': Dataset.from_dict({\n",
    "        'a': a70,\n",
    "        'b': b70\n",
    "    })\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['a', 'b'],\n",
       "    num_rows: 158003\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset.from_dict({\n",
    "    'a':a50,\n",
    "    'b':b50\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e943952b995a4eb4af8d5f52a3cb7722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071c8cb13ad040e5b6dbc549f4741988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/276 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2248adcc53a8453086c6adda07cd930d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43e40e299634de394c034efa1b52a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/235 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db68aab15a8c4718b42c9ed2eb7bc757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a651b2d8eaf24f489193afc9096f4fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/159 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9a4ae2e56c4665a83447e3cfd6266b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b60031db6bb4bb58716d54774b8c834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/lhallee/ProteinDoublesAll/commit/08bf2a1d4cc1b71740deed649f3259835e1083e3', commit_message='Upload dataset', commit_description='', oid='08bf2a1d4cc1b71740deed649f3259835e1083e3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.push_to_hub('lhallee/ProteinDoublesAll', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 2500\n",
      "Test set size: 2500\n"
     ]
    }
   ],
   "source": [
    "split = data.train_test_split(test_size=5000, seed=42)\n",
    "test_valid = split['test'].train_test_split(test_size=2500)\n",
    "valid_set = test_valid['train']\n",
    "test_set = test_valid['test']\n",
    "print(f\"Validation set size: {valid_set.num_rows}\")\n",
    "print(f\"Test set size: {test_set.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 153003\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['a', 'b'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_dataset = DatasetDict({\n",
    "    'train':split['train'],\n",
    "    'valid':valid_set,\n",
    "    'test':test_set\n",
    "})\n",
    "double_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c0433d087547878c77c016e89a0634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a33fecb66444a548e72e81db42db9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/154 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1333ca5dd28b4047962970abd7e1ce4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415a1c958aa241338c400ab99ddd3583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7e956e43f1436e9cea062b94ccbddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6aefa659af3487dac713489123a1319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/lhallee/ProteinDouble50/commit/e413c50bed0c77b9681a14a7ab42c2a6b62be4f6', commit_message='Upload dataset', commit_description='', oid='e413c50bed0c77b9681a14a7ab42c2a6b62be4f6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_dataset.push_to_hub('lhallee/ProteinDouble50', private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
